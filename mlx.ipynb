{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q mlx mlx-lm mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c28a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import sys, platform\n",
    "print('MLX', mx.__version__)\n",
    "print('Metal available', mx.metal.is_available())\n",
    "print('Python', sys.version)\n",
    "print('Platform', platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('distributed_infer.py','w') as f:\n",
    "    f.write('''import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "\n",
    "world = mx.distributed.init()\n",
    "rank = world.rank()\n",
    "size = world.size()\n",
    "mx.set_default_device(mx.gpu)\n",
    "\n",
    "if rank == 0:\n",
    "    print(f'Running on {size} processes')\n",
    "\n",
    "model, tokenizer = load('mlx-community/Llama-3.2-1B-Instruct-4bit')\n",
    "prompt = f'Hello from rank {rank}!'\n",
    "result = generate(model, tokenizer, prompt, max_tokens=20)\n",
    "print(f'[{rank}/{size} on {socket.gethostname()}] {result}')\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f6266",
   "metadata": {},
   "source": [
    "Run locally with two processes:\n",
    "```bash\n",
    "python -m mlx.launch --np 2 distributed_infer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb216b3",
   "metadata": {},
   "source": [
    "Run on multiple hosts using a hostfile:\n",
    "```bash\n",
    "python -m mlx.launch --hostfile hosts.txt distributed_infer.py\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
