{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Use the magic command without code block formatting\n",
    "# %pip install mlx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m ipykernel install --user --name mlx-distributed --display-name \"MLX Distributed (arm64)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f742fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove existing environment if it exists\n",
    "conda env remove -n mlx-distributed -y 2>/dev/null || true\n",
    "\n",
    "# Create fresh environment with Python 3.11 (optimal for MLX)\n",
    "CONDA_SUBDIR=osx-arm64 conda create -n mlx-distributed python=3.11 -y\n",
    "\n",
    "# Activate and configure for ARM64\n",
    "\n",
    "conda activate mlx-distributed\n",
    "conda config --env --set subdir osx-arm64\n",
    "\n",
    "echo \"Environment created successfully!\"\n",
    "conda info --envs | grep mlx-distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Activate environment\n",
    "source ~/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate mlx-distributed\n",
    "\n",
    "# Install OpenMPI via conda (not homebrew!)\n",
    "conda install -c conda-forge openmpi -y\n",
    "\n",
    "# Install mpi4py\n",
    "conda install -c conda-forge mpi4py -y\n",
    "\n",
    "# Install MLX and MLX-LM\n",
    "pip install mlx mlx-lm\n",
    "\n",
    "# Install additional utilities\n",
    "pip install numpy jupyter ipykernel\n",
    "\n",
    "# Add kernel to Jupyter\n",
    "python -m ipykernel install --user --name mlx-distributed --display-name \"MLX Distributed\"\n",
    "\n",
    "echo \"Installation complete!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3474418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print()\n",
    "\n",
    "print(\"=== MLX Installation ===\")\n",
    "try:\n",
    "    import mlx\n",
    "    import mlx.core as mx\n",
    "    print(f\"✓ MLX version: {mlx.__version__}\")\n",
    "    print(f\"✓ Metal available: {mx.metal.is_available()}\")\n",
    "    print(f\"✓ Default device: {mx.default_device()}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ MLX error: {e}\")\n",
    "print()\n",
    "\n",
    "print(\"=== MPI Installation ===\")\n",
    "try:\n",
    "    from mpi4py import MPI\n",
    "    print(f\"✓ mpi4py version: {MPI.Get_version()}\")\n",
    "    print(f\"✓ MPI vendor: {MPI.get_vendor()}\")\n",
    "    \n",
    "    # Check MPI executable\n",
    "    result = subprocess.run(['which', 'mpirun'], capture_output=True, text=True)\n",
    "    print(f\"✓ mpirun location: {result.stdout.strip()}\")\n",
    "    \n",
    "    # Check MPI version - fix for f-string issue\n",
    "    result = subprocess.run(['mpirun', '--version'], capture_output=True, text=True)\n",
    "    first_line = result.stdout.strip().split('\\n')[0]  # Move split outside f-string\n",
    "    print(f\"✓ MPI version: {first_line}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ MPI error: {e}\")\n",
    "print()\n",
    "\n",
    "print(\"=== MLX-LM Installation ===\")\n",
    "try:\n",
    "    import mlx_lm\n",
    "    print(\"✓ mlx_lm installed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ mlx_lm error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c05722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import time\n",
    "\n",
    "# Set GPU as default device\n",
    "mx.set_default_device(mx.gpu)\n",
    "\n",
    "print(\"=== GPU Test ===\")\n",
    "print(f\"Default device: {mx.default_device()}\")\n",
    "print(f\"Metal available: {mx.metal.is_available()}\")\n",
    "\n",
    "# Create a large array to test GPU\n",
    "size = 10000\n",
    "print(f\"\\nCreating {size}x{size} matrix multiplication...\")\n",
    "\n",
    "# Time CPU vs GPU\n",
    "start = time.time()\n",
    "a = mx.random.uniform(shape=(size, size))\n",
    "b = mx.random.uniform(shape=(size, size))\n",
    "c = a @ b\n",
    "mx.eval(c)  # Force evaluation\n",
    "gpu_time = time.time() - start\n",
    "\n",
    "print(f\"GPU computation time: {gpu_time:.3f} seconds\")\n",
    "print(f\"GPU memory used: {mx.metal.get_active_memory() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory cache: {mx.metal.get_cache_memory() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Test small model loading\n",
    "print(\"\\n=== Testing Model Loading ===\")\n",
    "try:\n",
    "    from mlx_lm import load\n",
    "    model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "    print(\"✓ Model loaded successfully\")\n",
    "    \n",
    "    # Quick inference test\n",
    "    prompt = \"Hello\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"np\")\n",
    "    print(f\"✓ Tokenizer works: '{prompt}' -> {inputs['input_ids']}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Model loading error: {e}\")\n",
    "    print(\"This is okay for now - we'll use a different model for distributed tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd0158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "hosts = [\"mm@mm1.local\", \"mm@mm2.local\"]\n",
    "\n",
    "print(\"=== Testing SSH Connectivity ===\")\n",
    "for host in hosts:\n",
    "    print(f\"\\nTesting {host}...\")\n",
    "    \n",
    "    # Test basic SSH\n",
    "    result = subprocess.run(\n",
    "        [\"ssh\", \"-o\", \"BatchMode=yes\", \"-o\", \"ConnectTimeout=5\", host, \"echo 'SSH OK'\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ SSH connection successful\")\n",
    "    else:\n",
    "        print(f\"✗ SSH connection failed: {result.stderr}\")\n",
    "        print(f\"  Fix: Run 'ssh-copy-id {host}' in terminal\")\n",
    "\n",
    "# Create SSH config for faster connections\n",
    "ssh_config = \"\"\"\n",
    "Host mm1.local\n",
    "    User mm\n",
    "    HostName mm1.local\n",
    "    ForwardAgent yes\n",
    "    ServerAliveInterval 60\n",
    "\n",
    "Host mm2.local\n",
    "    User mm\n",
    "    HostName mm2.local\n",
    "    ForwardAgent yes\n",
    "    ServerAliveInterval 60\n",
    "\n",
    "Host *\n",
    "    AddKeysToAgent yes\n",
    "    UseKeychain yes\n",
    "    IdentityFile ~/.ssh/id_rsa\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== Recommended SSH Config ===\")\n",
    "print(\"Add this to ~/.ssh/config:\")\n",
    "print(ssh_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd038fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Check if mpi4py is installed in current environment\n",
    "try:\n",
    "    import mpi4py\n",
    "    print(f\"✓ mpi4py is installed in current Python: {mpi4py.__file__}\")\n",
    "    print(f\"  mpi4py version: {mpi4py.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ mpi4py not found in current Python\")\n",
    "\n",
    "# Check which Python we're using\n",
    "print(f\"\\nCurrent Python: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Better way to check installed packages\n",
    "try:\n",
    "    import pkg_resources\n",
    "    installed_packages = [d.project_name for d in pkg_resources.working_set]\n",
    "    if 'mpi4py' in installed_packages:\n",
    "        version = pkg_resources.get_distribution('mpi4py').version\n",
    "        print(f\"\\n✓ mpi4py {version} is installed via pip\")\n",
    "    else:\n",
    "        print(\"\\n✗ mpi4py not found in pip packages\")\n",
    "except:\n",
    "    # Alternative method\n",
    "    import importlib.metadata\n",
    "    try:\n",
    "        version = importlib.metadata.version('mpi4py')\n",
    "        print(f\"\\n✓ mpi4py {version} is installed\")\n",
    "    except:\n",
    "        print(\"\\n✗ mpi4py not installed\")\n",
    "\n",
    "# Check conda list instead\n",
    "import subprocess\n",
    "result = subprocess.run(['conda', 'list', 'mpi4py'], capture_output=True, text=True)\n",
    "print(f\"\\nConda list output:\\n{result.stdout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304fc6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=== Solution: Use Homebrew MPI ===\")\n",
    "\n",
    "# First, uninstall the broken mpi4py\n",
    "print(\"1. Removing broken mpi4py...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'uninstall', 'mpi4py', '-y'])\n",
    "\n",
    "# Install mpi4py compiled against Homebrew's MPI\n",
    "print(\"\\n2. Installing mpi4py with Homebrew MPI...\")\n",
    "env = os.environ.copy()\n",
    "env['MPICC'] = '/opt/homebrew/bin/mpicc'\n",
    "env['CC'] = '/opt/homebrew/bin/mpicc'\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, '-m', 'pip', 'install', 'mpi4py', '--no-cache-dir', '--no-binary', 'mpi4py'],\n",
    "    capture_output=True, text=True, env=env\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ mpi4py installed successfully\")\n",
    "else:\n",
    "    print(f\"Installation output: {result.stdout}\")\n",
    "    print(f\"Errors: {result.stderr}\")\n",
    "\n",
    "# Test the installation\n",
    "print(\"\\n3. Testing MPI...\")\n",
    "test_script = \"\"\"\n",
    "import sys\n",
    "print(f\"Python: {sys.executable}\")\n",
    "\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(f\"Rank {rank}/{size}: MPI is working!\")\n",
    "\n",
    "if rank == 0 and size > 1:\n",
    "    comm.send(\"Hello from rank 0\", dest=1)\n",
    "elif rank == 1:\n",
    "    msg = comm.recv(source=0)\n",
    "    print(f\"Rank 1 received: {msg}\")\n",
    "\"\"\"\n",
    "\n",
    "with open('test_mpi_final.py', 'w') as f:\n",
    "    f.write(test_script)\n",
    "\n",
    "# Run with Homebrew's mpirun\n",
    "result = subprocess.run(\n",
    "    ['/opt/homebrew/bin/mpirun', '-np', '2', sys.executable, 'test_mpi_final.py'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "print(\"\\nOutput:\")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Errors:\", result.stderr)\n",
    "\n",
    "os.remove('test_mpi_final.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for using Homebrew MPI\n",
    "config_content = f\"\"\"#!/bin/bash\n",
    "# MLX Distributed Configuration\n",
    "\n",
    "# Use Homebrew MPI\n",
    "export PATH=\"/opt/homebrew/bin:$PATH\"\n",
    "export MPICC=/opt/homebrew/bin/mpicc\n",
    "export MPIRUN=/opt/homebrew/bin/mpirun\n",
    "\n",
    "# Python from conda environment\n",
    "export PYTHON={sys.executable}\n",
    "\n",
    "# Function to run distributed MLX\n",
    "run_mlx_dist() {{\n",
    "    /opt/homebrew/bin/mpirun \"$@\"\n",
    "}}\n",
    "\n",
    "echo \"MLX Distributed configured with:\"\n",
    "echo \"  MPI: Homebrew OpenMPI 5.0.7\"\n",
    "echo \"  Python: Conda environment (mlx-distributed)\"\n",
    "echo \"\"\n",
    "echo \"Usage: run_mlx_dist -np 4 python your_script.py\"\n",
    "\"\"\"\n",
    "\n",
    "with open('mlx_dist_config.sh', 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "os.chmod('mlx_dist_config.sh', 0o755)\n",
    "\n",
    "print(\"\\n=== Configuration Created ===\")\n",
    "print(\"Source this before running distributed jobs:\")\n",
    "print(\"  source mlx_dist_config.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "mlx_launch = '/Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch'\n",
    "\n",
    "print(\"=== Creating Final Working Scripts ===\")\n",
    "\n",
    "# Local run script with MPI backend\n",
    "local_script = f\"\"\"#!/bin/bash\n",
    "# Run MLX distributed locally with MPI backend\n",
    "\n",
    "NP=\"${{1:-2}}\"\n",
    "SCRIPT=\"${{2:-test_mlx_dist.py}}\"\n",
    "\n",
    "echo \"Running MLX locally with $NP processes (MPI backend)...\"\n",
    "echo \"Script: $SCRIPT\"\n",
    "echo \"\"\n",
    "\n",
    "{mlx_launch} --backend mpi --hosts localhost -n \"$NP\" \"$SCRIPT\"\n",
    "\"\"\"\n",
    "\n",
    "with open('run_mlx_local.sh', 'w') as f:\n",
    "    f.write(local_script)\n",
    "os.chmod('run_mlx_local.sh', 0o755)\n",
    "\n",
    "# Distributed run script for your cluster\n",
    "distributed_script = f\"\"\"#!/bin/bash\n",
    "# Run MLX distributed across your Mac cluster\n",
    "\n",
    "SCRIPT=\"${{1:-test_mlx_dist.py}}\"\n",
    "PROCESSES_PER_HOST=\"${{2:-2}}\"\n",
    "\n",
    "echo \"Running MLX distributed (MPI backend)\"\n",
    "echo \"Hosts: mbp.local, mm1.local, mm2.local\"\n",
    "echo \"Processes per host: $PROCESSES_PER_HOST\"\n",
    "echo \"Script: $SCRIPT\"\n",
    "echo \"\"\n",
    "\n",
    "{mlx_launch} --backend mpi \\\\\n",
    "    --hosts mbp.local,mm1.local,mm2.local \\\\\n",
    "    -n \"$PROCESSES_PER_HOST\" \\\\\n",
    "    \"$SCRIPT\"\n",
    "\"\"\"\n",
    "\n",
    "with open('run_mlx_distributed.sh', 'w') as f:\n",
    "    f.write(distributed_script)\n",
    "os.chmod('run_mlx_distributed.sh', 0o755)\n",
    "\n",
    "# Create hostfile for MPI backend\n",
    "hostfile_content = \"\"\"mbp.local\n",
    "mbp.local\n",
    "mm1.local\n",
    "mm1.local\n",
    "mm2.local\n",
    "mm2.local\n",
    "\"\"\"\n",
    "\n",
    "with open('mlx_hostfile.txt', 'w') as f:\n",
    "    f.write(hostfile_content)\n",
    "\n",
    "# Hostfile version\n",
    "hostfile_script = f\"\"\"#!/bin/bash\n",
    "# Run MLX using hostfile (MPI backend)\n",
    "\n",
    "SCRIPT=\"${{1:-test_mlx_dist.py}}\"\n",
    "HOSTFILE=\"${{2:-mlx_hostfile.txt}}\"\n",
    "\n",
    "echo \"Running MLX with hostfile (MPI backend)\"\n",
    "echo \"Hostfile: $HOSTFILE\"\n",
    "echo \"Script: $SCRIPT\"\n",
    "echo \"\"\n",
    "\n",
    "{mlx_launch} --backend mpi --hostfile \"$HOSTFILE\" \"$SCRIPT\"\n",
    "\"\"\"\n",
    "\n",
    "with open('run_mlx_hostfile.sh', 'w') as f:\n",
    "    f.write(hostfile_script)\n",
    "os.chmod('run_mlx_hostfile.sh', 0o755)\n",
    "\n",
    "print(\"Created working scripts!\")\n",
    "print(\"\\n✅ Test locally first:\")\n",
    "print(\"   ./run_mlx_local.sh 4\")\n",
    "print(\"\\n✅ Then run distributed:\")\n",
    "print(\"   ./run_mlx_distributed.sh\")\n",
    "print(\"   # This will run 2 processes on each of your 3 Macs (6 total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive distributed test\n",
    "comprehensive_test = \"\"\"\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import socket\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Initialize distributed\n",
    "world = mx.distributed.init()\n",
    "rank = world.rank()\n",
    "size = world.size()\n",
    "hostname = socket.gethostname()\n",
    "pid = os.getpid()\n",
    "\n",
    "# Set GPU\n",
    "mx.set_default_device(mx.gpu)\n",
    "\n",
    "print(f\"[Rank {rank}/{size}] Process {pid} on {hostname}\")\n",
    "print(f\"[Rank {rank}] GPU: {mx.metal.is_available()}\")\n",
    "print(f\"[Rank {rank}] Device: {mx.default_device()}\")\n",
    "\n",
    "# Synchronize before tests\n",
    "mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"Running MLX Distributed Tests\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Test 1: Basic all-reduce\n",
    "if rank == 0:\n",
    "    print(\"\\\\n1. Testing all-reduce...\")\n",
    "    \n",
    "local_value = mx.array([float(rank)])\n",
    "sum_result = mx.distributed.all_sum(local_value)\n",
    "mx.eval(sum_result)\n",
    "\n",
    "if rank == 0:\n",
    "    expected = sum(range(size))\n",
    "    print(f\"   All-reduce sum: {sum_result.item()} (expected: {expected})\")\n",
    "    print(f\"   {'✓ PASSED' if abs(sum_result.item() - expected) < 0.001 else '✗ FAILED'}\")\n",
    "\n",
    "# Test 2: Model parameter synchronization\n",
    "if rank == 0:\n",
    "    print(\"\\\\n2. Testing model parameter sync...\")\n",
    "\n",
    "model = nn.Linear(100, 10)\n",
    "mx.eval(model.parameters())\n",
    "\n",
    "# Get initial param sum\n",
    "param_sum_before = sum(p.sum().item() for _, p in model.parameters())\n",
    "print(f\"[Rank {rank}] Initial param sum: {param_sum_before:.6f}\")\n",
    "\n",
    "# Synchronize parameters\n",
    "for _, p in model.parameters():\n",
    "    p_synced = mx.distributed.all_sum(p) / size\n",
    "    p[:] = p_synced\n",
    "\n",
    "mx.eval(model.parameters())\n",
    "param_sum_after = sum(p.sum().item() for _, p in model.parameters())\n",
    "\n",
    "# All ranks should have same param sum now\n",
    "all_sums = mx.distributed.all_sum(mx.array([param_sum_after]))\n",
    "mx.eval(all_sums)\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"   Synchronized param sum: {param_sum_after:.6f}\")\n",
    "    print(f\"   {'✓ PASSED' if all_sums.item() == param_sum_after * size else '✗ FAILED'}\")\n",
    "\n",
    "# Test 3: Bandwidth test\n",
    "if rank == 0:\n",
    "    print(\"\\\\n3. Testing bandwidth...\")\n",
    "\n",
    "size_mb = 10\n",
    "data = mx.random.uniform(shape=(size_mb * 1024 * 1024 // 4,))\n",
    "\n",
    "start = time.time()\n",
    "result = mx.distributed.all_sum(data)\n",
    "mx.eval(result)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "bandwidth = size_mb * size / elapsed\n",
    "if rank == 0:\n",
    "    print(f\"   Data size: {size_mb}MB per rank\")\n",
    "    print(f\"   Time: {elapsed:.3f}s\")\n",
    "    print(f\"   Bandwidth: {bandwidth:.1f} MB/s\")\n",
    "\n",
    "# Final status\n",
    "mx.eval(mx.distributed.all_sum(mx.array([1.0])))  # Sync\n",
    "if rank == 0:\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"✓ All tests completed successfully!\")\n",
    "    print(\"=\"*50)\n",
    "\"\"\"\n",
    "\n",
    "with open('test_mlx_comprehensive.py', 'w') as f:\n",
    "    f.write(comprehensive_test)\n",
    "\n",
    "print(\"\\n=== Setup Complete! ===\")\n",
    "print(\"\\n🎉 MLX distributed is working correctly!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Test comprehensive script locally:\")\n",
    "print(\"   ./run_mlx_local.sh 4 test_mlx_comprehensive.py\")\n",
    "print(\"\\n2. Deploy environment to mm1.local and mm2.local\")\n",
    "print(\"   (They need the same mlx-distributed conda environment)\")\n",
    "print(\"\\n3. Run distributed across your cluster:\")\n",
    "print(\"   ./run_mlx_distributed.sh test_mlx_comprehensive.py\")\n",
    "print(\"\\nThis will run 6 processes total (2 on each Mac)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-distributed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
