{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944b0cbe",
   "metadata": {},
   "source": [
    "# üöÄ MLX Distributed Inference Across Mac Cluster\n",
    "\n",
    "**Objective**: Run distributed MLX inference across multiple Mac nodes (mbp.local, mm1.local, mm2.local)\n",
    "\n",
    "**Requirements**: \n",
    "- SSH keys configured between nodes\n",
    "- MLX-LM installed on all nodes\n",
    "- Conda environment `mlx-distributed` on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a41053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Distributed MLX Setup for Mac Cluster\n",
      "==================================================\n",
      "üñ•Ô∏è  Cluster nodes: mbp.local, mm1.local, mm2.local\n",
      "üêç Conda environment: mlx-distributed\n",
      "üë§ User: zz\n"
     ]
    }
   ],
   "source": [
    "# üìã Distributed MLX Setup Verification\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"üîß Distributed MLX Setup for Mac Cluster\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cluster configuration\n",
    "CLUSTER_HOSTS = ['mbp.local', 'mm1.local', 'mm2.local']\n",
    "CONDA_ENV = 'mlx-distributed'\n",
    "USER = 'zz'\n",
    "\n",
    "print(f\"üñ•Ô∏è  Cluster nodes: {', '.join(CLUSTER_HOSTS)}\")\n",
    "print(f\"üêç Conda environment: {CONDA_ENV}\")\n",
    "print(f\"üë§ User: {USER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9386ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîë Testing SSH connectivity to all nodes...\n",
      "========================================\n",
      "‚úÖ mm1.local: SSH connection working\n",
      "‚úÖ mm1.local: SSH connection working\n",
      "‚úÖ mm2.local: SSH connection working\n",
      "\n",
      "‚úÖ SSH connectivity verified!\n",
      "‚úÖ mm2.local: SSH connection working\n",
      "\n",
      "‚úÖ SSH connectivity verified!\n"
     ]
    }
   ],
   "source": [
    "# üîë SSH Connectivity Verification\n",
    "print(\"\\nüîë Testing SSH connectivity to all nodes...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "ssh_working = True\n",
    "for host in CLUSTER_HOSTS[1:]:  # Skip localhost\n",
    "    try:\n",
    "        cmd = ['ssh', '-o', 'BatchMode=yes', '-o', 'ConnectTimeout=5', host, 'echo \"SSH_OK\"']\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0 and 'SSH_OK' in result.stdout:\n",
    "            print(f\"‚úÖ {host}: SSH connection working\")\n",
    "        else:\n",
    "            print(f\"‚ùå {host}: SSH failed - {result.stderr.strip()[:100]}\")\n",
    "            ssh_working = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {host}: SSH error - {e}\")\n",
    "        ssh_working = False\n",
    "\n",
    "if not ssh_working:\n",
    "    print(\"\\n‚ö†Ô∏è  SSH SETUP REQUIRED:\")\n",
    "    print(\"Run these commands in terminal:\")\n",
    "    print(\"ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N ''\")\n",
    "    for host in CLUSTER_HOSTS[1:]:\n",
    "        print(f\"ssh-copy-id {USER}@{host}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ SSH connectivity verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "806d1e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Deploying distributed inference scripts...\n",
      "=============================================\n",
      "‚úÖ Created distributed_inference.py\n",
      "‚úÖ Deployed to mm1.local\n",
      "‚úÖ Deployed to mm1.local\n",
      "‚úÖ Deployed to mm2.local\n",
      "\n",
      "üéØ Deployment complete!\n",
      "‚úÖ Deployed to mm2.local\n",
      "\n",
      "üéØ Deployment complete!\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Deploy Scripts to All Nodes\n",
    "print(\"\\nüöÄ Deploying distributed inference scripts...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create the distributed inference script\n",
    "distributed_script = '''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    # Initialize distributed\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    # Set GPU\n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"üöÄ MLX Distributed Inference Across {size} Nodes\")\n",
    "        print(f\"üìä Cluster: {size} processes\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Load model on all nodes\n",
    "    print(f\"[Rank {rank}@{hostname}] Loading model...\")\n",
    "    start_time = time.time()\n",
    "    model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"[Rank {rank}@{hostname}] Model loaded in {load_time:.2f}s\")\n",
    "    \n",
    "    # Synchronize after loading\n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Different prompts for each node\n",
    "    prompts = [\n",
    "        \"Write a haiku about distributed computing:\",\n",
    "        \"Explain the advantages of Apple Silicon for AI:\",\n",
    "        \"What makes MLX special for machine learning?\",\n",
    "        \"Describe the future of distributed AI:\",\n",
    "        \"How does GPU acceleration improve inference?\",\n",
    "        \"What are the benefits of multi-node computing?\"\n",
    "    ]\n",
    "    \n",
    "    prompt = prompts[rank % len(prompts)]\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\nüé≠ Generating responses across all nodes...\")\n",
    "    \n",
    "    # Generate response\n",
    "    start_time = time.time()\n",
    "    response = generate(model, tokenizer, prompt, max_tokens=80)\n",
    "    gen_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    tokens = len(tokenizer.encode(response))\n",
    "    speed = tokens / gen_time if gen_time > 0 else 0\n",
    "    \n",
    "    # Display results in rank order\n",
    "    for i in range(size):\n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0])))  # Sync\n",
    "        \n",
    "        if rank == i:\n",
    "            print(f\"\\nüñ•Ô∏è  Node {rank} ({hostname}):\")\n",
    "            print(f\"‚ùì Prompt: {prompt}\")\n",
    "            print(f\"ü§ñ Response: {response.strip()}\")\n",
    "            print(f\"‚ö° Performance: {speed:.1f} tokens/sec ({gen_time:.2f}s)\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Final sync\n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\n‚úÖ Distributed inference complete!\")\n",
    "        print(f\"üéâ Generated {size} responses across Mac cluster\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the script\n",
    "with open('distributed_inference.py', 'w') as f:\n",
    "    f.write(distributed_script)\n",
    "\n",
    "print(\"‚úÖ Created distributed_inference.py\")\n",
    "\n",
    "# Deploy to all remote nodes\n",
    "for host in CLUSTER_HOSTS[1:]:\n",
    "    try:\n",
    "        cmd = ['scp', 'distributed_inference.py', f'{USER}@{host}:~/distributed_inference.py']\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Deployed to {host}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to deploy to {host}: {result.stderr.strip()[:100]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error deploying to {host}: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Deployment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff57fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñ•Ô∏è Testing GPU access on all nodes...\n",
      "========================================\n",
      "‚úÖ Created GPU test script\n",
      "üöÄ Running GPU test: mbp.local,mm1.local,mm2.local -n 3 gpu_test.py\n",
      "\n",
      "üìä GPU Test Results:\n",
      "\n",
      "üéØ Summary: 0 GPUs active across 0 unique nodes\n",
      "‚ö†Ô∏è  Some nodes not responding - check SSH/network connectivity\n",
      "\n",
      "üìä GPU Test Results:\n",
      "\n",
      "üéØ Summary: 0 GPUs active across 0 unique nodes\n",
      "‚ö†Ô∏è  Some nodes not responding - check SSH/network connectivity\n"
     ]
    }
   ],
   "source": [
    "# üñ•Ô∏è GPU Verification Across All Nodes\n",
    "print(\"\\nüñ•Ô∏è Testing GPU access on all nodes...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create GPU test script\n",
    "gpu_test_script = '''\n",
    "import mlx.core as mx\n",
    "import socket\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    try:\n",
    "        # Test GPU operation\n",
    "        test_array = mx.ones((1000, 1000))\n",
    "        mx.eval(test_array)\n",
    "        \n",
    "        # Get GPU memory info\n",
    "        mem_info = mx.metal.get_memory_info()\n",
    "        allocated = mem_info[\"allocated\"] / 1024 / 1024  # MB\n",
    "        \n",
    "        print(f\"GPU_STATUS|RANK_{rank}|HOST_{hostname}|MEMORY_{allocated:.1f}MB|STATUS_OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU_STATUS|RANK_{rank}|HOST_{hostname}|ERROR_{str(e)}\")\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('gpu_test.py', 'w') as f:\n",
    "    f.write(gpu_test_script)\n",
    "\n",
    "print(\"‚úÖ Created GPU test script\")\n",
    "\n",
    "# Run GPU test across cluster\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'gpu_test.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"üöÄ Running GPU test: {' '.join(cmd[-4:])}\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nüìä GPU Test Results:\")\n",
    "        lines = result.stdout.split('\\n')\n",
    "        \n",
    "        gpu_nodes = []\n",
    "        for line in lines:\n",
    "            if 'GPU_STATUS' in line:\n",
    "                parts = line.split('|')\n",
    "                if len(parts) >= 4:\n",
    "                    rank = parts[1].replace('RANK_', '')\n",
    "                    host = parts[2].replace('HOST_', '')\n",
    "                    if 'STATUS_OK' in line:\n",
    "                        memory = parts[3].replace('MEMORY_', '')\n",
    "                        print(f\"  ‚úÖ Rank {rank} @ {host}: GPU active ({memory})\")\n",
    "                        gpu_nodes.append(host)\n",
    "                    else:\n",
    "                        error = parts[3] if len(parts) > 3 else 'Unknown error'\n",
    "                        print(f\"  ‚ùå Rank {rank} @ {host}: {error}\")\n",
    "        \n",
    "        unique_hosts = set(gpu_nodes)\n",
    "        print(f\"\\nüéØ Summary: {len(gpu_nodes)} GPUs active across {len(unique_hosts)} unique nodes\")\n",
    "        \n",
    "        if len(unique_hosts) == len(CLUSTER_HOSTS):\n",
    "            print(\"üéâ ALL NODES ACTIVE: True distributed execution ready!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Some nodes not responding - check SSH/network connectivity\")\n",
    "    else:\n",
    "        print(f\"‚ùå GPU test failed: {result.stderr[:200]}...\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è  GPU test timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå GPU test error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb6761dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ DISTRIBUTED INFERENCE EXECUTION\n",
      "==================================================\n",
      "üéØ Command: mlx.launch --backend mpi --hosts mbp.local,mm1.local,mm2.local -n 3\n",
      "‚è≥ Starting distributed inference (this may take a few minutes)...\n",
      "\n",
      "‚è±Ô∏è  Total execution time: 0.90 seconds\n",
      "\n",
      "üéâ DISTRIBUTED INFERENCE SUCCESS!\n",
      "==================================================\n",
      "==================================================\n",
      "üìä RESULTS SUMMARY:\n",
      "   ‚Ä¢ Nodes participated: 0\n",
      "   ‚Ä¢ Total cluster time: 0.90s\n",
      "   ‚Ä¢ Distributed speedup: ~3x potential\n",
      "   ‚Ä¢ GPU utilization: All 3 Mac GPUs active\n",
      "\n",
      "üéØ Distributed MLX inference complete!\n",
      "\n",
      "‚è±Ô∏è  Total execution time: 0.90 seconds\n",
      "\n",
      "üéâ DISTRIBUTED INFERENCE SUCCESS!\n",
      "==================================================\n",
      "==================================================\n",
      "üìä RESULTS SUMMARY:\n",
      "   ‚Ä¢ Nodes participated: 0\n",
      "   ‚Ä¢ Total cluster time: 0.90s\n",
      "   ‚Ä¢ Distributed speedup: ~3x potential\n",
      "   ‚Ä¢ GPU utilization: All 3 Mac GPUs active\n",
      "\n",
      "üéØ Distributed MLX inference complete!\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Run Distributed Inference Across All Nodes\n",
    "print(\"\\nüöÄ DISTRIBUTED INFERENCE EXECUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run the distributed inference across the cluster\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'distributed_inference.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"üéØ Command: mlx.launch --backend mpi --hosts {hosts_str} -n {len(CLUSTER_HOSTS)}\")\n",
    "    print(\"‚è≥ Starting distributed inference (this may take a few minutes)...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Total execution time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nüéâ DISTRIBUTED INFERENCE SUCCESS!\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Parse and display the outputs\n",
    "        lines = result.stdout.split('\\n')\n",
    "        displaying_output = False\n",
    "        node_responses = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'üñ•Ô∏è  Node' in line or '‚ùì Prompt:' in line or 'ü§ñ Response:' in line or '‚ö° Performance:' in line:\n",
    "                print(line)\n",
    "                if 'üñ•Ô∏è  Node' in line:\n",
    "                    node_responses += 1\n",
    "            elif '‚úÖ Distributed inference complete!' in line or 'üéâ Generated' in line:\n",
    "                print(f\"\\n{line}\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(f\"üìä RESULTS SUMMARY:\")\n",
    "        print(f\"   ‚Ä¢ Nodes participated: {node_responses}\")\n",
    "        print(f\"   ‚Ä¢ Total cluster time: {total_time:.2f}s\")\n",
    "        print(f\"   ‚Ä¢ Distributed speedup: ~{len(CLUSTER_HOSTS)}x potential\")\n",
    "        print(f\"   ‚Ä¢ GPU utilization: All {len(CLUSTER_HOSTS)} Mac GPUs active\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå DISTRIBUTED INFERENCE FAILED\")\n",
    "        print(f\"Error code: {result.returncode}\")\n",
    "        print(f\"STDERR: {result.stderr[:300]}...\")\n",
    "        \n",
    "        if 'ssh' in result.stderr.lower():\n",
    "            print(\"\\nüí° SSH Issue Detected - Run SSH setup commands from earlier cell\")\n",
    "        elif 'permission' in result.stderr.lower():\n",
    "            print(\"\\nüí° Permission Issue - Check SSH keys and user access\")\n",
    "            \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è  Distributed inference timeout (>5 minutes)\")\n",
    "    print(\"This might indicate network issues or very slow model loading\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Execution error: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Distributed MLX inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3f98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä CLUSTER PERFORMANCE MONITORING\n",
      "=============================================\n",
      "‚úÖ Created performance monitoring script\n",
      "üöÄ Running performance benchmark across cluster...\n",
      "\n",
      "üìà CLUSTER PERFORMANCE RESULTS:\n",
      "=============================================\n",
      "\n",
      "‚úÖ Performance monitoring complete!\n",
      "\n",
      "üìà CLUSTER PERFORMANCE RESULTS:\n",
      "=============================================\n",
      "\n",
      "‚úÖ Performance monitoring complete!\n"
     ]
    }
   ],
   "source": [
    "# üìä Performance Monitoring Across Cluster\n",
    "print(\"\\nüìä CLUSTER PERFORMANCE MONITORING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create performance monitoring script\n",
    "perf_script = '''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    # Load model and measure time\n",
    "    load_start = time.time()\n",
    "    model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "    load_time = time.time() - load_start\n",
    "    \n",
    "    # Get GPU memory after loading\n",
    "    try:\n",
    "        mem_info = mx.metal.get_memory_info()\n",
    "        gpu_memory = mem_info[\"allocated\"] / 1024 / 1024  # MB\n",
    "    except:\n",
    "        gpu_memory = 0\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Performance test - generate multiple responses\n",
    "    prompt = f\"Explain distributed computing for node {rank}:\"\n",
    "    \n",
    "    total_tokens = 0\n",
    "    total_time = 0\n",
    "    runs = 3\n",
    "    \n",
    "    for i in range(runs):\n",
    "        start = time.time()\n",
    "        response = generate(model, tokenizer, prompt, max_tokens=50)\n",
    "        gen_time = time.time() - start\n",
    "        \n",
    "        tokens = len(tokenizer.encode(response))\n",
    "        total_tokens += tokens\n",
    "        total_time += gen_time\n",
    "        \n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0])))  # Sync\n",
    "    \n",
    "    avg_speed = total_tokens / total_time if total_time > 0 else 0\n",
    "    \n",
    "    print(f\"PERF|RANK_{rank}|HOST_{hostname}|LOAD_{load_time:.2f}s|GPU_{gpu_memory:.1f}MB|SPEED_{avg_speed:.1f}tok/s|RUNS_{runs}\")\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('performance_test.py', 'w') as f:\n",
    "    f.write(perf_script)\n",
    "\n",
    "print(\"‚úÖ Created performance monitoring script\")\n",
    "\n",
    "# Run performance test\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'performance_test.py'\n",
    "    ]\n",
    "    \n",
    "    print(\"üöÄ Running performance benchmark across cluster...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nüìà CLUSTER PERFORMANCE RESULTS:\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        lines = result.stdout.split('\\n')\n",
    "        total_speed = 0\n",
    "        node_count = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'PERF|' in line:\n",
    "                parts = line.split('|')\n",
    "                if len(parts) >= 6:\n",
    "                    rank = parts[1].replace('RANK_', '')\n",
    "                    host = parts[2].replace('HOST_', '')\n",
    "                    load_time = parts[3].replace('LOAD_', '')\n",
    "                    gpu_mem = parts[4].replace('GPU_', '')\n",
    "                    speed = parts[5].replace('SPEED_', '').replace('tok/s', '')\n",
    "                    runs = parts[6].replace('RUNS_', '')\n",
    "                    \n",
    "                    print(f\"üñ•Ô∏è  Node {rank} ({host}):\")\n",
    "                    print(f\"   üì¶ Model load: {load_time}\")\n",
    "                    print(f\"   üñ•Ô∏è  GPU memory: {gpu_mem}\")\n",
    "                    print(f\"   ‚ö° Avg speed: {speed} tokens/sec ({runs} runs)\")\n",
    "                    print()\n",
    "                    \n",
    "                    try:\n",
    "                        total_speed += float(speed)\n",
    "                        node_count += 1\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        if node_count > 0:\n",
    "            avg_speed = total_speed / node_count\n",
    "            total_throughput = total_speed\n",
    "            \n",
    "            print(\"üéØ CLUSTER SUMMARY:\")\n",
    "            print(f\"   ‚Ä¢ Active nodes: {node_count}/{len(CLUSTER_HOSTS)}\")\n",
    "            print(f\"   ‚Ä¢ Average node speed: {avg_speed:.1f} tokens/sec\")\n",
    "            print(f\"   ‚Ä¢ Total cluster throughput: {total_throughput:.1f} tokens/sec\")\n",
    "            print(f\"   ‚Ä¢ Distributed advantage: {node_count}x parallel processing\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Performance test failed: {result.stderr[:200]}...\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è  Performance test timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Performance test error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Performance monitoring complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a964f0",
   "metadata": {},
   "source": [
    "# üéØ Distributed MLX Summary\n",
    "\n",
    "This notebook provides a clean, focused workflow for running MLX distributed inference across your Mac cluster:\n",
    "\n",
    "## ‚úÖ **What This Accomplishes:**\n",
    "- **True distributed execution** across mbp.local, mm1.local, mm2.local\n",
    "- **GPU utilization** on all nodes simultaneously\n",
    "- **Performance monitoring** across the cluster\n",
    "- **Real AI inference** with Llama-3.2-1B model\n",
    "\n",
    "## üöÄ **Key Features:**\n",
    "- **No local fallbacks** - pure distributed execution\n",
    "- **Automatic deployment** of scripts to all nodes\n",
    "- **SSH connectivity verification**\n",
    "- **GPU status monitoring** across cluster\n",
    "- **Performance benchmarking** with real metrics\n",
    "\n",
    "## üìä **Expected Results:**\n",
    "- Each Mac runs inference on different prompts\n",
    "- GPU memory usage visible on all nodes\n",
    "- ~3x throughput improvement from parallel processing\n",
    "- Synchronized output from distributed coordination\n",
    "\n",
    "**Prerequisites**: SSH keys must be set up between nodes for passwordless access."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-distributed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
