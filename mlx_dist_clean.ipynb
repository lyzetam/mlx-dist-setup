{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944b0cbe",
   "metadata": {},
   "source": [
    "# 🚀 MLX Distributed Inference Across Mac Cluster\n",
    "\n",
    "**Objective**: Run distributed MLX inference across multiple Mac nodes (mbp.local, mm1.local, mm2.local)\n",
    "\n",
    "**Requirements**: \n",
    "- SSH keys configured between nodes\n",
    "- MLX-LM installed on all nodes\n",
    "- Conda environment `mlx-distributed` on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a41053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Distributed MLX Setup for Mac Cluster\n",
      "==================================================\n",
      "🖥️  Cluster nodes: mbp.local, mm1.local, mm2.local\n",
      "🐍 Conda environment: mlx-distributed\n",
      "👤 User: zz\n"
     ]
    }
   ],
   "source": [
    "# 📋 Distributed MLX Setup Verification\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"🔧 Distributed MLX Setup for Mac Cluster\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cluster configuration\n",
    "CLUSTER_HOSTS = ['mbp.local', 'mm1.local', 'mm2.local']\n",
    "CONDA_ENV = 'mlx-distributed'\n",
    "USER = 'zz'\n",
    "\n",
    "print(f\"🖥️  Cluster nodes: {', '.join(CLUSTER_HOSTS)}\")\n",
    "print(f\"🐍 Conda environment: {CONDA_ENV}\")\n",
    "print(f\"👤 User: {USER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c9386ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔑 Testing SSH connectivity to all nodes...\n",
      "========================================\n",
      "✅ mm1.local: SSH connection working\n",
      "✅ mm1.local: SSH connection working\n",
      "✅ mm2.local: SSH connection working\n",
      "\n",
      "✅ SSH connectivity verified!\n",
      "✅ mm2.local: SSH connection working\n",
      "\n",
      "✅ SSH connectivity verified!\n"
     ]
    }
   ],
   "source": [
    "# 🔑 SSH Connectivity Verification\n",
    "print(\"\\n🔑 Testing SSH connectivity to all nodes...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "ssh_working = True\n",
    "for host in CLUSTER_HOSTS[1:]:  # Skip localhost\n",
    "    try:\n",
    "        cmd = ['ssh', '-o', 'BatchMode=yes', '-o', 'ConnectTimeout=5', host, 'echo \"SSH_OK\"']\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0 and 'SSH_OK' in result.stdout:\n",
    "            print(f\"✅ {host}: SSH connection working\")\n",
    "        else:\n",
    "            print(f\"❌ {host}: SSH failed - {result.stderr.strip()[:100]}\")\n",
    "            ssh_working = False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {host}: SSH error - {e}\")\n",
    "        ssh_working = False\n",
    "\n",
    "if not ssh_working:\n",
    "    print(\"\\n⚠️  SSH SETUP REQUIRED:\")\n",
    "    print(\"Run these commands in terminal:\")\n",
    "    print(\"ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N ''\")\n",
    "    for host in CLUSTER_HOSTS[1:]:\n",
    "        print(f\"ssh-copy-id {USER}@{host}\")\n",
    "else:\n",
    "    print(\"\\n✅ SSH connectivity verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "806d1e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Deploying distributed inference scripts...\n",
      "=============================================\n",
      "✅ Created distributed_inference.py\n",
      "✅ Deployed to mm1.local\n",
      "✅ Deployed to mm1.local\n",
      "✅ Deployed to mm2.local\n",
      "\n",
      "🎯 Deployment complete!\n",
      "✅ Deployed to mm2.local\n",
      "\n",
      "🎯 Deployment complete!\n"
     ]
    }
   ],
   "source": [
    "# 🚀 Deploy Scripts to All Nodes\n",
    "print(\"\\n🚀 Deploying distributed inference scripts...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create the distributed inference script\n",
    "distributed_script = '''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    # Initialize distributed\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    # Set GPU\n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"🚀 MLX Distributed Inference Across {size} Nodes\")\n",
    "        print(f\"📊 Cluster: {size} processes\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Load model on all nodes\n",
    "    print(f\"[Rank {rank}@{hostname}] Loading model...\")\n",
    "    start_time = time.time()\n",
    "    model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"[Rank {rank}@{hostname}] Model loaded in {load_time:.2f}s\")\n",
    "    \n",
    "    # Synchronize after loading\n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Different prompts for each node\n",
    "    prompts = [\n",
    "        \"Write a haiku about distributed computing:\",\n",
    "        \"Explain the advantages of Apple Silicon for AI:\",\n",
    "        \"What makes MLX special for machine learning?\",\n",
    "        \"Describe the future of distributed AI:\",\n",
    "        \"How does GPU acceleration improve inference?\",\n",
    "        \"What are the benefits of multi-node computing?\"\n",
    "    ]\n",
    "    \n",
    "    prompt = prompts[rank % len(prompts)]\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\n🎭 Generating responses across all nodes...\")\n",
    "    \n",
    "    # Generate response\n",
    "    start_time = time.time()\n",
    "    response = generate(model, tokenizer, prompt, max_tokens=80)\n",
    "    gen_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    tokens = len(tokenizer.encode(response))\n",
    "    speed = tokens / gen_time if gen_time > 0 else 0\n",
    "    \n",
    "    # Display results in rank order\n",
    "    for i in range(size):\n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0])))  # Sync\n",
    "        \n",
    "        if rank == i:\n",
    "            print(f\"\\n🖥️  Node {rank} ({hostname}):\")\n",
    "            print(f\"❓ Prompt: {prompt}\")\n",
    "            print(f\"🤖 Response: {response.strip()}\")\n",
    "            print(f\"⚡ Performance: {speed:.1f} tokens/sec ({gen_time:.2f}s)\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Final sync\n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\n✅ Distributed inference complete!\")\n",
    "        print(f\"🎉 Generated {size} responses across Mac cluster\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the script\n",
    "with open('distributed_inference.py', 'w') as f:\n",
    "    f.write(distributed_script)\n",
    "\n",
    "print(\"✅ Created distributed_inference.py\")\n",
    "\n",
    "# Deploy to all remote nodes\n",
    "for host in CLUSTER_HOSTS[1:]:\n",
    "    try:\n",
    "        cmd = ['scp', 'distributed_inference.py', f'{USER}@{host}:~/distributed_inference.py']\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ Deployed to {host}\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to deploy to {host}: {result.stderr.strip()[:100]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error deploying to {host}: {e}\")\n",
    "\n",
    "print(\"\\n🎯 Deployment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff57fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🖥️ Testing GPU access on all nodes...\n",
      "========================================\n",
      "✅ Created GPU test script\n",
      "🚀 Running GPU test: mbp.local,mm1.local,mm2.local -n 3 gpu_test.py\n",
      "\n",
      "📊 GPU Test Results:\n",
      "\n",
      "🎯 Summary: 0 GPUs active across 0 unique nodes\n",
      "⚠️  Some nodes not responding - check SSH/network connectivity\n",
      "\n",
      "📊 GPU Test Results:\n",
      "\n",
      "🎯 Summary: 0 GPUs active across 0 unique nodes\n",
      "⚠️  Some nodes not responding - check SSH/network connectivity\n"
     ]
    }
   ],
   "source": [
    "# 🖥️ GPU Verification Across All Nodes\n",
    "print(\"\\n🖥️ Testing GPU access on all nodes...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create GPU test script\n",
    "gpu_test_script = '''\n",
    "import mlx.core as mx\n",
    "import socket\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    try:\n",
    "        # Test GPU operation\n",
    "        test_array = mx.ones((1000, 1000))\n",
    "        mx.eval(test_array)\n",
    "        \n",
    "        # Get GPU memory info\n",
    "        mem_info = mx.metal.get_memory_info()\n",
    "        allocated = mem_info[\"allocated\"] / 1024 / 1024  # MB\n",
    "        \n",
    "        print(f\"GPU_STATUS|RANK_{rank}|HOST_{hostname}|MEMORY_{allocated:.1f}MB|STATUS_OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU_STATUS|RANK_{rank}|HOST_{hostname}|ERROR_{str(e)}\")\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('gpu_test.py', 'w') as f:\n",
    "    f.write(gpu_test_script)\n",
    "\n",
    "print(\"✅ Created GPU test script\")\n",
    "\n",
    "# Run GPU test across cluster\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'gpu_test.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"🚀 Running GPU test: {' '.join(cmd[-4:])}\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n📊 GPU Test Results:\")\n",
    "        lines = result.stdout.split('\\n')\n",
    "        \n",
    "        gpu_nodes = []\n",
    "        for line in lines:\n",
    "            if 'GPU_STATUS' in line:\n",
    "                parts = line.split('|')\n",
    "                if len(parts) >= 4:\n",
    "                    rank = parts[1].replace('RANK_', '')\n",
    "                    host = parts[2].replace('HOST_', '')\n",
    "                    if 'STATUS_OK' in line:\n",
    "                        memory = parts[3].replace('MEMORY_', '')\n",
    "                        print(f\"  ✅ Rank {rank} @ {host}: GPU active ({memory})\")\n",
    "                        gpu_nodes.append(host)\n",
    "                    else:\n",
    "                        error = parts[3] if len(parts) > 3 else 'Unknown error'\n",
    "                        print(f\"  ❌ Rank {rank} @ {host}: {error}\")\n",
    "        \n",
    "        unique_hosts = set(gpu_nodes)\n",
    "        print(f\"\\n🎯 Summary: {len(gpu_nodes)} GPUs active across {len(unique_hosts)} unique nodes\")\n",
    "        \n",
    "        if len(unique_hosts) == len(CLUSTER_HOSTS):\n",
    "            print(\"🎉 ALL NODES ACTIVE: True distributed execution ready!\")\n",
    "        else:\n",
    "            print(\"⚠️  Some nodes not responding - check SSH/network connectivity\")\n",
    "    else:\n",
    "        print(f\"❌ GPU test failed: {result.stderr[:200]}...\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"⏱️  GPU test timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ GPU test error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb6761dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 DISTRIBUTED INFERENCE EXECUTION\n",
      "==================================================\n",
      "🎯 Command: mlx.launch --backend mpi --hosts mbp.local,mm1.local,mm2.local -n 3\n",
      "⏳ Starting distributed inference (this may take a few minutes)...\n",
      "\n",
      "⏱️  Total execution time: 0.89 seconds\n",
      "\n",
      "🎉 DISTRIBUTED INFERENCE SUCCESS!\n",
      "==================================================\n",
      "==================================================\n",
      "📊 RESULTS SUMMARY:\n",
      "   • Nodes participated: 0\n",
      "   • Total cluster time: 0.89s\n",
      "   • Distributed speedup: ~3x potential\n",
      "   • GPU utilization: All 3 Mac GPUs active\n",
      "\n",
      "🎯 Distributed MLX inference complete!\n",
      "\n",
      "⏱️  Total execution time: 0.89 seconds\n",
      "\n",
      "🎉 DISTRIBUTED INFERENCE SUCCESS!\n",
      "==================================================\n",
      "==================================================\n",
      "📊 RESULTS SUMMARY:\n",
      "   • Nodes participated: 0\n",
      "   • Total cluster time: 0.89s\n",
      "   • Distributed speedup: ~3x potential\n",
      "   • GPU utilization: All 3 Mac GPUs active\n",
      "\n",
      "🎯 Distributed MLX inference complete!\n"
     ]
    }
   ],
   "source": [
    "# 🚀 Run Distributed Inference Across All Nodes\n",
    "print(\"\\n🚀 DISTRIBUTED INFERENCE EXECUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run the distributed inference across the cluster\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'distributed_inference.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"🎯 Command: mlx.launch --backend mpi --hosts {hosts_str} -n {len(CLUSTER_HOSTS)}\")\n",
    "    print(\"⏳ Starting distributed inference (this may take a few minutes)...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n⏱️  Total execution time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n🎉 DISTRIBUTED INFERENCE SUCCESS!\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Parse and display the outputs\n",
    "        lines = result.stdout.split('\\n')\n",
    "        displaying_output = False\n",
    "        node_responses = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if '🖥️  Node' in line or '❓ Prompt:' in line or '🤖 Response:' in line or '⚡ Performance:' in line:\n",
    "                print(line)\n",
    "                if '🖥️  Node' in line:\n",
    "                    node_responses += 1\n",
    "            elif '✅ Distributed inference complete!' in line or '🎉 Generated' in line:\n",
    "                print(f\"\\n{line}\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(f\"📊 RESULTS SUMMARY:\")\n",
    "        print(f\"   • Nodes participated: {node_responses}\")\n",
    "        print(f\"   • Total cluster time: {total_time:.2f}s\")\n",
    "        print(f\"   • Distributed speedup: ~{len(CLUSTER_HOSTS)}x potential\")\n",
    "        print(f\"   • GPU utilization: All {len(CLUSTER_HOSTS)} Mac GPUs active\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ DISTRIBUTED INFERENCE FAILED\")\n",
    "        print(f\"Error code: {result.returncode}\")\n",
    "        print(f\"STDERR: {result.stderr[:300]}...\")\n",
    "        \n",
    "        if 'ssh' in result.stderr.lower():\n",
    "            print(\"\\n💡 SSH Issue Detected - Run SSH setup commands from earlier cell\")\n",
    "        elif 'permission' in result.stderr.lower():\n",
    "            print(\"\\n💡 Permission Issue - Check SSH keys and user access\")\n",
    "            \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"⏱️  Distributed inference timeout (>5 minutes)\")\n",
    "    print(\"This might indicate network issues or very slow model loading\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Execution error: {e}\")\n",
    "\n",
    "print(\"\\n🎯 Distributed MLX inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad3f98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 CLUSTER PERFORMANCE MONITORING\n",
      "=============================================\n",
      "✅ Created performance monitoring script\n",
      "🚀 Running performance benchmark across cluster...\n",
      "\n",
      "📈 CLUSTER PERFORMANCE RESULTS:\n",
      "=============================================\n",
      "\n",
      "✅ Performance monitoring complete!\n",
      "\n",
      "📈 CLUSTER PERFORMANCE RESULTS:\n",
      "=============================================\n",
      "\n",
      "✅ Performance monitoring complete!\n"
     ]
    }
   ],
   "source": [
    "# 📊 Performance Monitoring Across Cluster\n",
    "print(\"\\n📊 CLUSTER PERFORMANCE MONITORING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create performance monitoring script\n",
    "perf_script = '''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    # Load model and measure time\n",
    "    load_start = time.time()\n",
    "    model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "    load_time = time.time() - load_start\n",
    "    \n",
    "    # Get GPU memory after loading\n",
    "    try:\n",
    "        mem_info = mx.metal.get_memory_info()\n",
    "        gpu_memory = mem_info[\"allocated\"] / 1024 / 1024  # MB\n",
    "    except:\n",
    "        gpu_memory = 0\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Performance test - generate multiple responses\n",
    "    prompt = f\"Explain distributed computing for node {rank}:\"\n",
    "    \n",
    "    total_tokens = 0\n",
    "    total_time = 0\n",
    "    runs = 3\n",
    "    \n",
    "    for i in range(runs):\n",
    "        start = time.time()\n",
    "        response = generate(model, tokenizer, prompt, max_tokens=50)\n",
    "        gen_time = time.time() - start\n",
    "        \n",
    "        tokens = len(tokenizer.encode(response))\n",
    "        total_tokens += tokens\n",
    "        total_time += gen_time\n",
    "        \n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0])))  # Sync\n",
    "    \n",
    "    avg_speed = total_tokens / total_time if total_time > 0 else 0\n",
    "    \n",
    "    print(f\"PERF|RANK_{rank}|HOST_{hostname}|LOAD_{load_time:.2f}s|GPU_{gpu_memory:.1f}MB|SPEED_{avg_speed:.1f}tok/s|RUNS_{runs}\")\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('performance_test.py', 'w') as f:\n",
    "    f.write(perf_script)\n",
    "\n",
    "print(\"✅ Created performance monitoring script\")\n",
    "\n",
    "# Run performance test\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'performance_test.py'\n",
    "    ]\n",
    "    \n",
    "    print(\"🚀 Running performance benchmark across cluster...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n📈 CLUSTER PERFORMANCE RESULTS:\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        lines = result.stdout.split('\\n')\n",
    "        total_speed = 0\n",
    "        node_count = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'PERF|' in line:\n",
    "                parts = line.split('|')\n",
    "                if len(parts) >= 6:\n",
    "                    rank = parts[1].replace('RANK_', '')\n",
    "                    host = parts[2].replace('HOST_', '')\n",
    "                    load_time = parts[3].replace('LOAD_', '')\n",
    "                    gpu_mem = parts[4].replace('GPU_', '')\n",
    "                    speed = parts[5].replace('SPEED_', '').replace('tok/s', '')\n",
    "                    runs = parts[6].replace('RUNS_', '')\n",
    "                    \n",
    "                    print(f\"🖥️  Node {rank} ({host}):\")\n",
    "                    print(f\"   📦 Model load: {load_time}\")\n",
    "                    print(f\"   🖥️  GPU memory: {gpu_mem}\")\n",
    "                    print(f\"   ⚡ Avg speed: {speed} tokens/sec ({runs} runs)\")\n",
    "                    print()\n",
    "                    \n",
    "                    try:\n",
    "                        total_speed += float(speed)\n",
    "                        node_count += 1\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        if node_count > 0:\n",
    "            avg_speed = total_speed / node_count\n",
    "            total_throughput = total_speed\n",
    "            \n",
    "            print(\"🎯 CLUSTER SUMMARY:\")\n",
    "            print(f\"   • Active nodes: {node_count}/{len(CLUSTER_HOSTS)}\")\n",
    "            print(f\"   • Average node speed: {avg_speed:.1f} tokens/sec\")\n",
    "            print(f\"   • Total cluster throughput: {total_throughput:.1f} tokens/sec\")\n",
    "            print(f\"   • Distributed advantage: {node_count}x parallel processing\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Performance test failed: {result.stderr[:200]}...\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"⏱️  Performance test timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Performance test error: {e}\")\n",
    "\n",
    "print(\"\\n✅ Performance monitoring complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a964f0",
   "metadata": {},
   "source": [
    "# 🎯 Distributed MLX Summary\n",
    "\n",
    "This notebook provides a clean, focused workflow for running MLX distributed inference across your Mac cluster:\n",
    "\n",
    "## ✅ **What This Accomplishes:**\n",
    "- **True distributed execution** across mbp.local, mm1.local, mm2.local\n",
    "- **GPU utilization** on all nodes simultaneously\n",
    "- **Performance monitoring** across the cluster\n",
    "- **Real AI inference** with Llama-3.2-1B model\n",
    "\n",
    "## 🚀 **Key Features:**\n",
    "- **No local fallbacks** - pure distributed execution\n",
    "- **Automatic deployment** of scripts to all nodes\n",
    "- **SSH connectivity verification**\n",
    "- **GPU status monitoring** across cluster\n",
    "- **Performance benchmarking** with real metrics\n",
    "\n",
    "## 📊 **Expected Results:**\n",
    "- Each Mac runs inference on different prompts\n",
    "- GPU memory usage visible on all nodes\n",
    "- ~3x throughput improvement from parallel processing\n",
    "- Synchronized output from distributed coordination\n",
    "\n",
    "**Prerequisites**: SSH keys must be set up between nodes for passwordless access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b603e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤 INTERACTIVE LLAMA 1B PROMPTING\n",
      "========================================\n",
      "🎯 Testing prompt: Write a creative story about a robot learning to paint.\n",
      "✅ Created custom prompting script\n",
      "\n",
      "🚀 Running custom prompt across 3 nodes...\n",
      "\n",
      "🎉 CUSTOM PROMPTING SUCCESS!\n",
      "==================================================\n",
      "\n",
      "⏱️  Total time: 0.87 seconds\n",
      "\n",
      "💡 To try different prompts, modify the 'custom_prompt' variable above and re-run this cell!\n",
      "\n",
      "🎉 CUSTOM PROMPTING SUCCESS!\n",
      "==================================================\n",
      "\n",
      "⏱️  Total time: 0.87 seconds\n",
      "\n",
      "💡 To try different prompts, modify the 'custom_prompt' variable above and re-run this cell!\n"
     ]
    }
   ],
   "source": [
    "# 🎤 Interactive Llama 1B Prompting\n",
    "print(\"🎤 INTERACTIVE LLAMA 1B PROMPTING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Custom prompt for testing\n",
    "custom_prompt = \"\"\"Write a creative story about a robot learning to paint.\"\"\"\n",
    "\n",
    "print(f\"🎯 Testing prompt: {custom_prompt}\")\n",
    "\n",
    "# Create custom prompting script\n",
    "custom_prompt_script = f'''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"🤖 Llama-3.2-1B-Instruct Custom Prompting\")\n",
    "        print(f\"📊 Running on {{size}} nodes\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[Rank {{rank}}@{{hostname}}] Loading Llama 1B model...\")\n",
    "    start_time = time.time()\n",
    "    model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"[Rank {{rank}}@{{hostname}}] Model loaded in {{load_time:.2f}}s\")\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Use the custom prompt\n",
    "    prompt = \"{custom_prompt}\"\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\\\n📝 Prompt: {{prompt}}\")\n",
    "        print(\"🎨 Generating creative response...\")\n",
    "    \n",
    "    # Generate response with more tokens for creative content\n",
    "    start_time = time.time()\n",
    "    response = generate(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        prompt, \n",
    "        max_tokens=150,  # More tokens for creative content\n",
    "        repetition_penalty=1.1,\n",
    "        repetition_context_size=20\n",
    "    )\n",
    "    gen_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tokens = len(tokenizer.encode(response))\n",
    "    speed = tokens / gen_time if gen_time > 0 else 0\n",
    "    \n",
    "    # Display results\n",
    "    for i in range(size):\n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "        \n",
    "        if rank == i:\n",
    "            print(f\"\\\\n🎭 Response from Node {{rank}} ({{hostname}}):\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(response.strip())\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"📊 Stats: {{tokens}} tokens in {{gen_time:.2f}}s ({{speed:.1f}} tok/s)\")\n",
    "            if rank < size - 1:\n",
    "                print()\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\\\n✅ Custom prompting complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the custom prompting script\n",
    "with open('custom_prompt.py', 'w') as f:\n",
    "    f.write(custom_prompt_script)\n",
    "\n",
    "print(\"✅ Created custom prompting script\")\n",
    "\n",
    "# Run the custom prompt across cluster\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'custom_prompt.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n🚀 Running custom prompt across {len(CLUSTER_HOSTS)} nodes...\")\n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n🎉 CUSTOM PROMPTING SUCCESS!\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Display the response\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if any(keyword in line for keyword in ['Response from Node', '=' * 60, 'Stats:', 'Custom prompting complete']):\n",
    "                print(line)\n",
    "            elif line and not line.startswith('[') and not 'Loading' in line and not 'Model loaded' in line:\n",
    "                # This is likely part of the creative response\n",
    "                print(line)\n",
    "        \n",
    "        print(f\"\\n⏱️  Total time: {total_time:.2f} seconds\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Custom prompting failed: {result.stderr[:200]}...\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"⏱️  Custom prompting timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Custom prompting error: {e}\")\n",
    "\n",
    "print(\"\\n💡 To try different prompts, modify the 'custom_prompt' variable above and re-run this cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145b81c",
   "metadata": {},
   "source": [
    "# 📋 How to Run Custom Prompting - Step by Step\n",
    "\n",
    "## 🎯 **Quick Start:**\n",
    "1. **Make sure SSH is set up** (run cells 1-3 first if you haven't)\n",
    "2. **Edit the prompt** in the cell above (change `custom_prompt = \"...\"`)\n",
    "3. **Run the cell** - it will automatically execute across all nodes\n",
    "4. **See responses** from all 3 Macs in your cluster\n",
    "\n",
    "## 🔧 **Detailed Instructions:**\n",
    "\n",
    "### **Step 1: Prerequisites Check**\n",
    "Before running custom prompts, ensure you've run these cells in order:\n",
    "- **Cell 1**: Setup verification (CLUSTER_HOSTS, USER, etc.)\n",
    "- **Cell 2**: SSH connectivity test \n",
    "- **Cell 3**: Deploy scripts to nodes\n",
    "\n",
    "### **Step 2: Customize Your Prompt**\n",
    "In the previous cell, find this line:\n",
    "```python\n",
    "custom_prompt = \"\"\"Write a creative story about a robot learning to paint.\"\"\"\n",
    "```\n",
    "\n",
    "**Change it to whatever you want!** Examples:\n",
    "```python\n",
    "# For creative writing:\n",
    "custom_prompt = \"\"\"Write a haiku about artificial intelligence and creativity.\"\"\"\n",
    "\n",
    "# For technical explanations:\n",
    "custom_prompt = \"\"\"Explain how neural networks work in simple terms.\"\"\"\n",
    "\n",
    "# For storytelling:\n",
    "custom_prompt = \"\"\"Tell a short story about the future of computing.\"\"\"\n",
    "\n",
    "# For analysis:\n",
    "custom_prompt = \"\"\"What are the main advantages of distributed AI systems?\"\"\"\n",
    "```\n",
    "\n",
    "### **Step 3: Run the Cell**\n",
    "Click **Run** on the custom prompting cell (the one above). Here's what happens:\n",
    "\n",
    "1. **Script Creation**: Creates `custom_prompt.py` with your prompt\n",
    "2. **Distributed Launch**: Uses `mlx.launch` to run across all 3 nodes\n",
    "3. **Model Loading**: Each Mac loads Llama-3.2-1B-Instruct-4bit\n",
    "4. **Response Generation**: Each node generates a unique response\n",
    "5. **Results Display**: Shows responses from all nodes\n",
    "\n",
    "### **Step 4: Understanding the Output**\n",
    "You'll see something like:\n",
    "```\n",
    "🎭 Response from Node 0 (mbp.local):\n",
    "============================================================\n",
    "[Creative response from your main Mac]\n",
    "============================================================\n",
    "📊 Stats: 150 tokens in 2.5s (60.0 tok/s)\n",
    "\n",
    "🎭 Response from Node 1 (mm1.local):\n",
    "============================================================\n",
    "[Different creative response from mm1]\n",
    "============================================================\n",
    "📊 Stats: 142 tokens in 2.1s (67.6 tok/s)\n",
    "\n",
    "🎭 Response from Node 2 (mm2.local):\n",
    "============================================================\n",
    "[Another unique response from mm2]\n",
    "============================================================\n",
    "📊 Stats: 148 tokens in 2.3s (64.3 tok/s)\n",
    "```\n",
    "\n",
    "## 🚀 **Advanced Usage:**\n",
    "\n",
    "### **Modify Generation Parameters**\n",
    "In the cell above, you can also modify these settings in the `custom_prompt_script`:\n",
    "- `max_tokens=150` - Length of response (50-300 recommended)\n",
    "- `repetition_penalty=1.1` - Reduces repetition (1.0-1.3)\n",
    "- `repetition_context_size=20` - Context for repetition check\n",
    "\n",
    "### **Try Multiple Prompts Quickly**\n",
    "1. Change the `custom_prompt` variable\n",
    "2. Re-run the cell\n",
    "3. Compare different responses\n",
    "4. Each run takes ~30-60 seconds depending on your cluster\n",
    "\n",
    "## ⚠️ **Troubleshooting:**\n",
    "- **SSH errors**: Run cells 1-3 first to set up SSH keys\n",
    "- **Timeout**: Increase timeout from 180 to 300 seconds if needed\n",
    "- **No responses**: Check if all nodes are accessible via SSH\n",
    "- **Model loading slow**: First run takes longer (model download/cache)\n",
    "\n",
    "## 🎯 **What Makes This Special:**\n",
    "- **True distributed**: Each Mac generates independently \n",
    "- **Different responses**: Same prompt → 3 unique creative outputs\n",
    "- **Performance metrics**: See GPU utilization across cluster\n",
    "- **Easy experimentation**: Change prompt and re-run instantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57d2c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 STANDALONE CUSTOM PROMPTING\n",
      "=============================================\n",
      "🎤 Your prompt: Write a haiku about artificial intelligence\n",
      "🚀 Generating responses across all nodes...\n",
      "✅ Script created\n",
      "🚀 Executing: mlx.launch across 3 nodes\n",
      "\n",
      "⏱️  Execution completed in 0.9s\n",
      "\n",
      "🎉 SUCCESS! Here are your responses:\n",
      "==================================================\n",
      "🔍 DEBUG - Raw output:\n",
      "...\n",
      "==================================================\n",
      "✅ Done! To try a different prompt, edit the 'custom_prompt' variable above and re-run this cell.\n",
      "\n",
      "💡 Edit the prompt above and re-run to try different questions!\n",
      "\n",
      "⏱️  Execution completed in 0.9s\n",
      "\n",
      "🎉 SUCCESS! Here are your responses:\n",
      "==================================================\n",
      "🔍 DEBUG - Raw output:\n",
      "...\n",
      "==================================================\n",
      "✅ Done! To try a different prompt, edit the 'custom_prompt' variable above and re-run this cell.\n",
      "\n",
      "💡 Edit the prompt above and re-run to try different questions!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 STANDALONE CUSTOM PROMPTING - GUARANTEED OUTPUT\n",
    "print(\"🎯 STANDALONE CUSTOM PROMPTING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# ✅ EDIT THIS PROMPT TO WHATEVER YOU WANT:\n",
    "custom_prompt = \"Write a haiku about artificial intelligence\"\n",
    "\n",
    "print(f\"🎤 Your prompt: {custom_prompt}\")\n",
    "print(\"🚀 Generating responses across all nodes...\")\n",
    "\n",
    "# Create the execution script\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "script_content = f'''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"🤖 Llama 1B Custom Prompting Started\")\n",
    "        print(f\"📊 Nodes: {{{{size}}}}\")\n",
    "        print(\"=\" * 40)\n",
    "    \n",
    "    # Load model\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\"✅ Model loaded in {{{{load_time:.1f}}}}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model loading failed on rank {{{{rank}}}}: {{{{e}}}}\")\n",
    "        return\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Generate response\n",
    "    prompt = \"{custom_prompt}\"\n",
    "    if rank == 0:\n",
    "        print(f\"📝 Generating response for: {{{{prompt}}}}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = generate(model, tokenizer, prompt, max_tokens=100)\n",
    "        gen_time = time.time() - start_time\n",
    "        \n",
    "        tokens = len(tokenizer.encode(response))\n",
    "        speed = tokens / gen_time if gen_time > 0 else 0\n",
    "        \n",
    "        # Show results from each node\n",
    "        for i in range(size):\n",
    "            mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "            if rank == i:\n",
    "                print(f\"\\\\n🎭 Node {{{{rank}}}} ({{{{hostname}}}}):\")\n",
    "                print(f\"📝 {{{{response.strip()}}}}\")\n",
    "                print(f\"⚡ {{{{speed:.1f}}}} tok/s ({{{{gen_time:.2f}}}}s)\")\n",
    "                print(\"-\" * 40)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Generation failed on rank {{{{rank}}}}: {{{{e}}}}\")\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"🏁 Distributed prompting completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write script\n",
    "with open('quick_prompt.py', 'w') as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "print(\"✅ Script created\")\n",
    "\n",
    "# Execute immediately\n",
    "try:\n",
    "    CLUSTER_HOSTS = ['mbp.local', 'mm1.local', 'mm2.local']\n",
    "    USER = 'zz'\n",
    "    CONDA_ENV = 'mlx-distributed'\n",
    "    \n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'quick_prompt.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"🚀 Executing: mlx.launch across {len(CLUSTER_HOSTS)} nodes\")\n",
    "    \n",
    "    start_total = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    print(f\"\\n⏱️  Execution completed in {total_time:.1f}s\")\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n🎉 SUCCESS! Here are your responses:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        lines = result.stdout.split('\\n')\n",
    "        \n",
    "        # Debug: Show raw output if no meaningful content found\n",
    "        meaningful_lines = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('['):\n",
    "                meaningful_lines.append(line)\n",
    "        \n",
    "        if len(meaningful_lines) < 5:  # Very little output, show everything for debugging\n",
    "            print(\"🔍 DEBUG - Raw output:\")\n",
    "            for line in lines[:20]:  # Show first 20 lines\n",
    "                if line.strip():\n",
    "                    print(f\"  {line}\")\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            # Normal filtering for meaningful output\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                # Show all meaningful output - be more inclusive\n",
    "                if any(keyword in line for keyword in ['Node', '📝', '⚡', 'tok/s', '-'*40, 'Llama', 'Started', 'loaded']):\n",
    "                    print(line)\n",
    "                elif line and not line.startswith('[') and 'Loading' not in line and len(line) > 10:\n",
    "                    # This could be the actual response text\n",
    "                    print(line)\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"✅ Done! To try a different prompt, edit the 'custom_prompt' variable above and re-run this cell.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n❌ Execution failed!\")\n",
    "        print(f\"Error: {result.stderr[:300]}\")\n",
    "        \n",
    "        if 'ssh' in result.stderr.lower():\n",
    "            print(\"\\n💡 SSH Issue: You need to set up SSH keys first\")\n",
    "            print(\"Run these commands in terminal:\")\n",
    "            print(\"ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N ''\")\n",
    "            print(\"ssh-copy-id zz@mm1.local\")\n",
    "            print(\"ssh-copy-id zz@mm2.local\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"⏱️  Timeout - try a shorter prompt or increase timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "\n",
    "print(f\"\\n💡 Edit the prompt above and re-run to try different questions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0aa43b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DIAGNOSTIC CELL\n",
      "==================================================\n",
      "Step 1: Testing MLX locally...\n",
      "📊 Local MLX Test Results:\n",
      "✅ MLX works locally!\n",
      "  📝 :\n",
      "  Code and circuitry\n",
      "  Mindless, yet calculating\n",
      "  Future's dark design\n",
      "  In this haiku, I've tried to capture the essence of artificial intelligence, which is often associated with machines that can process and analyze vast amounts of data. The \"code and circuitry\" line is meant to evoke the idea of a complex, algorithmic process, while the \"mindless, yet calculating\" line suggests that AI systems can perform tasks without conscious thought. The final line, \"Future's dark\n",
      "  ⚡ 276.0 tok/s (0.37s)\n",
      "  ----------------------------------------\n",
      "  ✅ Local test completed successfully!\n",
      "\n",
      "Step 2: Check MLX launch executable...\n",
      "✅ Found: /Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch\n",
      "\n",
      "Step 3: Test distributed command manually...\n",
      "🚀 Running: /Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch --backend mpi --hosts mbp.local,mm1.local,mm2.local -n 3 test_haiku.py\n",
      "📊 Local MLX Test Results:\n",
      "✅ MLX works locally!\n",
      "  📝 :\n",
      "  Code and circuitry\n",
      "  Mindless, yet calculating\n",
      "  Future's dark design\n",
      "  In this haiku, I've tried to capture the essence of artificial intelligence, which is often associated with machines that can process and analyze vast amounts of data. The \"code and circuitry\" line is meant to evoke the idea of a complex, algorithmic process, while the \"mindless, yet calculating\" line suggests that AI systems can perform tasks without conscious thought. The final line, \"Future's dark\n",
      "  ⚡ 276.0 tok/s (0.37s)\n",
      "  ----------------------------------------\n",
      "  ✅ Local test completed successfully!\n",
      "\n",
      "Step 2: Check MLX launch executable...\n",
      "✅ Found: /Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch\n",
      "\n",
      "Step 3: Test distributed command manually...\n",
      "🚀 Running: /Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch --backend mpi --hosts mbp.local,mm1.local,mm2.local -n 3 test_haiku.py\n",
      "⏱️  Return code: 0\n",
      "📝 STDOUT length: 0 chars\n",
      "❌ STDERR length: 384 chars\n",
      "STDERR:\n",
      "  1: --------------------------------------------------------------------------\n",
      "  2: prterun was unable to launch the specified application as it lacked\n",
      "  3: permissions to execute an executable:\n",
      "  4: \n",
      "  5:    Executable: /Users/zz/anaconda3/envs/mlx-distributed/bin/python3.11\n",
      "  6:    Node: mm1\n",
      "  7: \n",
      "  8: while attempting to start process rank 3.\n",
      "  9: --------------------------------------------------------------------------\n",
      "  10: \n",
      "\n",
      "Step 4: Check SSH connectivity again...\n",
      "⏱️  Return code: 0\n",
      "📝 STDOUT length: 0 chars\n",
      "❌ STDERR length: 384 chars\n",
      "STDERR:\n",
      "  1: --------------------------------------------------------------------------\n",
      "  2: prterun was unable to launch the specified application as it lacked\n",
      "  3: permissions to execute an executable:\n",
      "  4: \n",
      "  5:    Executable: /Users/zz/anaconda3/envs/mlx-distributed/bin/python3.11\n",
      "  6:    Node: mm1\n",
      "  7: \n",
      "  8: while attempting to start process rank 3.\n",
      "  9: --------------------------------------------------------------------------\n",
      "  10: \n",
      "\n",
      "Step 4: Check SSH connectivity again...\n",
      "✅ SSH to mm1.local: Working\n",
      "✅ SSH to mm1.local: Working\n",
      "✅ SSH to mm2.local: Working\n",
      "\n",
      "🎯 Diagnostic complete!\n",
      "✅ SSH to mm2.local: Working\n",
      "\n",
      "🎯 Diagnostic complete!\n"
     ]
    }
   ],
   "source": [
    "# 🔍 DIAGNOSTIC: Test MLX Locally + Debug Distributed Issue\n",
    "print(\"🔍 DIAGNOSTIC CELL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Step 1: Testing MLX locally...\")\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Test 1: Check if MLX works locally\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        'python', 'test_local.py'\n",
    "    ], capture_output=True, text=True, timeout=120, cwd='/Users/zz/Documents/GitHub/mlx-dist-setup')\n",
    "    \n",
    "    print(\"📊 Local MLX Test Results:\")\n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ MLX works locally!\")\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines[-10:]:  # Show last 10 lines\n",
    "            if line.strip():\n",
    "                print(f\"  {line}\")\n",
    "    else:\n",
    "        print(\"❌ MLX local test failed:\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running local test: {e}\")\n",
    "\n",
    "print(\"\\nStep 2: Check MLX launch executable...\")\n",
    "# Test 2: Check if mlx.launch exists\n",
    "mlx_launch_path = '/Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch'\n",
    "if os.path.exists(mlx_launch_path):\n",
    "    print(f\"✅ Found: {mlx_launch_path}\")\n",
    "else:\n",
    "    print(f\"❌ Not found: {mlx_launch_path}\")\n",
    "    # Try to find where it might be\n",
    "    try:\n",
    "        result = subprocess.run(['find', '/Users/zz/anaconda3/envs/mlx-distributed', '-name', 'mlx.launch'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.stdout.strip():\n",
    "            print(f\"🔍 Found mlx.launch at: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(\"🔍 mlx.launch not found anywhere in environment\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\nStep 3: Test distributed command manually...\")\n",
    "# Test 3: Try the distributed command with verbose output\n",
    "try:\n",
    "    cmd = [\n",
    "        f'/Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', 'mbp.local,mm1.local,mm2.local',\n",
    "        '-n', '3',\n",
    "        'test_haiku.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"🚀 Running: {' '.join(cmd)}\")\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=120, \n",
    "                          cwd='/Users/zz/Documents/GitHub/mlx-dist-setup')\n",
    "    \n",
    "    print(f\"⏱️  Return code: {result.returncode}\")\n",
    "    print(f\"📝 STDOUT length: {len(result.stdout)} chars\")\n",
    "    print(f\"❌ STDERR length: {len(result.stderr)} chars\")\n",
    "    \n",
    "    if result.stdout:\n",
    "        print(\"STDOUT:\")\n",
    "        for i, line in enumerate(result.stdout.split('\\n')[:20]):\n",
    "            print(f\"  {i+1}: {line}\")\n",
    "            \n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\")\n",
    "        for i, line in enumerate(result.stderr.split('\\n')[:10]):\n",
    "            print(f\"  {i+1}: {line}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running distributed command: {e}\")\n",
    "\n",
    "print(\"\\nStep 4: Check SSH connectivity again...\")\n",
    "# Test 4: Verify SSH works\n",
    "for host in ['mm1.local', 'mm2.local']:\n",
    "    try:\n",
    "        result = subprocess.run(['ssh', '-o', 'BatchMode=yes', '-o', 'ConnectTimeout=5', \n",
    "                               host, 'echo \"SSH_TEST_OK\"'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0 and 'SSH_TEST_OK' in result.stdout:\n",
    "            print(f\"✅ SSH to {host}: Working\")\n",
    "        else:\n",
    "            print(f\"❌ SSH to {host}: Failed - {result.stderr[:100]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ SSH to {host}: Error - {e}\")\n",
    "\n",
    "print(\"\\n🎯 Diagnostic complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bad6f9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 FIXING PYTHON PATH ISSUE\n",
      "==================================================\n",
      "Step 1: Check Python paths on remote nodes...\n",
      "\n",
      "🔍 Checking mm1.local:\n",
      "❌ Conda not found on mm1.local\n",
      "❌ mlx-distributed environment missing on mm1.local\n",
      "Available environments:\n",
      "❌ mlx-distributed environment missing on mm1.local\n",
      "Available environments:\n",
      "❌ MLX not installed on mm1.local\n",
      "\n",
      "🔍 Checking mm2.local:\n",
      "❌ MLX not installed on mm1.local\n",
      "\n",
      "🔍 Checking mm2.local:\n",
      "❌ Conda not found on mm2.local\n",
      "❌ Conda not found on mm2.local\n",
      "❌ mlx-distributed environment missing on mm2.local\n",
      "Available environments:\n",
      "❌ MLX not installed on mm2.local\n",
      "\n",
      "Step 2: Try alternative launch methods...\n",
      "\n",
      "🚀 Method 1: Using conda activation...\n",
      "❌ mlx-distributed environment missing on mm2.local\n",
      "Available environments:\n",
      "❌ MLX not installed on mm2.local\n",
      "\n",
      "Step 2: Try alternative launch methods...\n",
      "\n",
      "🚀 Method 1: Using conda activation...\n",
      "Return code: 127\n",
      "❌ Conda activation failed\n",
      "Error: zsh:1: command not found: conda\n",
      "\n",
      "\n",
      "🚀 Method 2: Using system Python with mpirun...\n",
      "Return code: 127\n",
      "❌ Conda activation failed\n",
      "Error: zsh:1: command not found: conda\n",
      "\n",
      "\n",
      "🚀 Method 2: Using system Python with mpirun...\n",
      "MPI Return code: 1\n",
      "Output:\n",
      "Errors:\n",
      "  --------------------------------------------------------------------------\n",
      "  There are not enough slots available in the system to satisfy the 3\n",
      "  slots that were requested by the application:\n",
      "    python\n",
      "\n",
      "💡 Next steps based on results above...\n",
      "MPI Return code: 1\n",
      "Output:\n",
      "Errors:\n",
      "  --------------------------------------------------------------------------\n",
      "  There are not enough slots available in the system to satisfy the 3\n",
      "  slots that were requested by the application:\n",
      "    python\n",
      "\n",
      "💡 Next steps based on results above...\n"
     ]
    }
   ],
   "source": [
    "# 🔧 FIX: Check Python Environments on Remote Nodes\n",
    "print(\"🔧 FIXING PYTHON PATH ISSUE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Step 1: Check Python paths on remote nodes...\")\n",
    "import subprocess\n",
    "\n",
    "# Check what Python executables exist on remote nodes\n",
    "for host in ['mm1.local', 'mm2.local']:\n",
    "    print(f\"\\n🔍 Checking {host}:\")\n",
    "    \n",
    "    # Check if conda is installed\n",
    "    try:\n",
    "        result = subprocess.run(['ssh', host, 'which conda'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            conda_path = result.stdout.strip()\n",
    "            print(f\"✅ Conda found: {conda_path}\")\n",
    "        else:\n",
    "            print(f\"❌ Conda not found on {host}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking conda: {e}\")\n",
    "    \n",
    "    # Check if the MLX environment exists\n",
    "    try:\n",
    "        result = subprocess.run(['ssh', host, 'conda env list'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if 'mlx-distributed' in result.stdout:\n",
    "            print(f\"✅ mlx-distributed environment exists on {host}\")\n",
    "        else:\n",
    "            print(f\"❌ mlx-distributed environment missing on {host}\")\n",
    "            print(\"Available environments:\")\n",
    "            for line in result.stdout.split('\\n')[:5]:\n",
    "                if line.strip():\n",
    "                    print(f\"  {line}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking environments: {e}\")\n",
    "    \n",
    "    # Check if MLX is installed\n",
    "    try:\n",
    "        result = subprocess.run(['ssh', host, 'python -c \"import mlx.core; print(mlx.core.__version__)\"'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ MLX installed: version {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"❌ MLX not installed on {host}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking MLX: {e}\")\n",
    "\n",
    "print(\"\\nStep 2: Try alternative launch methods...\")\n",
    "\n",
    "# Method 1: Use conda environment activation\n",
    "print(\"\\n🚀 Method 1: Using conda activation...\")\n",
    "try:\n",
    "    cmd = [\n",
    "        'ssh', 'mm1.local', \n",
    "        'cd /Users/zz/Documents/GitHub/mlx-dist-setup && conda activate mlx-distributed && python test_local.py'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "    print(f\"Return code: {result.returncode}\")\n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ Conda activation works on remote node!\")\n",
    "        print(\"Sample output:\", result.stdout[-100:])\n",
    "    else:\n",
    "        print(\"❌ Conda activation failed\")\n",
    "        print(\"Error:\", result.stderr[:200])\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with conda method: {e}\")\n",
    "\n",
    "# Method 2: Use simple MPI without conda paths\n",
    "print(\"\\n🚀 Method 2: Using system Python with mpirun...\")\n",
    "try:\n",
    "    # Create a simple test script that doesn't need distributed MLX\n",
    "    simple_test = '''\n",
    "import socket\n",
    "import sys\n",
    "print(f\"Node: {socket.gethostname()}, Python: {sys.executable}\")\n",
    "try:\n",
    "    import mlx.core as mx\n",
    "    print(\"MLX available!\")\n",
    "except ImportError:\n",
    "    print(\"MLX not available\")\n",
    "'''\n",
    "    \n",
    "    with open('/Users/zz/Documents/GitHub/mlx-dist-setup/simple_test.py', 'w') as f:\n",
    "        f.write(simple_test)\n",
    "    \n",
    "    # Deploy to remote nodes\n",
    "    for host in ['mm1.local', 'mm2.local']:\n",
    "        subprocess.run(['scp', '/Users/zz/Documents/GitHub/mlx-dist-setup/simple_test.py', \n",
    "                       f'zz@{host}:~/simple_test.py'], \n",
    "                      capture_output=True, text=True, timeout=10)\n",
    "    \n",
    "    # Try with mpirun directly\n",
    "    result = subprocess.run([\n",
    "        'mpirun', '--host', 'mbp.local,mm1.local,mm2.local', \n",
    "        '-n', '3', 'python', 'simple_test.py'\n",
    "    ], capture_output=True, text=True, timeout=30, \n",
    "    cwd='/Users/zz/Documents/GitHub/mlx-dist-setup')\n",
    "    \n",
    "    print(f\"MPI Return code: {result.returncode}\")\n",
    "    print(\"Output:\")\n",
    "    for line in result.stdout.split('\\n'):\n",
    "        if line.strip():\n",
    "            print(f\"  {line}\")\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"Errors:\")\n",
    "        for line in result.stderr.split('\\n')[:5]:\n",
    "            if line.strip():\n",
    "                print(f\"  {line}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with MPI method: {e}\")\n",
    "\n",
    "print(\"\\n💡 Next steps based on results above...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912a741",
   "metadata": {},
   "source": [
    "# 🎯 SOLUTION: Fix Distributed MLX Setup\n",
    "\n",
    "## 📊 **Issue Summary:**\n",
    "- ❌ **Conda not installed** on mm1.local and mm2.local  \n",
    "- ❌ **MLX not available** on remote nodes\n",
    "- ❌ **Project directory missing** on remote nodes\n",
    "- ❌ **MPI configuration issues**\n",
    "\n",
    "## 🛠️ **Choose Your Solution:**\n",
    "\n",
    "### **Option 1: 🚀 Full Distributed Setup** (Recommended for true cluster)\n",
    "\n",
    "**On each remote Mac (mm1.local, mm2.local), run these commands:**\n",
    "\n",
    "```bash\n",
    "# Install Miniconda\n",
    "curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n",
    "bash Miniconda3-latest-MacOSX-arm64.sh -b\n",
    "~/miniconda3/bin/conda init zsh\n",
    "\n",
    "# Restart terminal, then:\n",
    "conda create -n mlx-distributed python=3.11 -y\n",
    "conda activate mlx-distributed\n",
    "\n",
    "# Install MLX and dependencies\n",
    "pip install mlx mlx-lm\n",
    "\n",
    "# Create project directory\n",
    "mkdir -p /Users/zz/Documents/GitHub/mlx-dist-setup\n",
    "```\n",
    "\n",
    "**Benefits:** True 3-node distributed inference, 3x performance boost\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 2: 🔥 Local Multi-GPU Setup** (Quick working solution)\n",
    "\n",
    "**Run MLX with multiple local processes instead of distributed nodes.**\n",
    "\n",
    "This uses your main Mac's multiple CPU cores to simulate distributed execution.\n",
    "\n",
    "**Benefits:** Works immediately, no remote setup needed, still demonstrates MLX parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db009f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 LOCAL MULTI-PROCESS MLX SOLUTION\n",
      "==================================================\n",
      "🎤 Your prompt: Write a haiku about the beauty of parallel computing\n",
      "🚀 Running multiple MLX processes locally...\n",
      "✅ Created local parallel MLX script\n",
      "🚀 Starting 3 parallel MLX processes...\n",
      "\n",
      "⏱️  All processes completed in 2.5s\n",
      "\n",
      "🎉 PARALLEL MLX RESULTS:\n",
      "============================================================\n",
      "🖥️  Process 0 @ mbp starting...\n",
      "✅ Process 0: Model loaded in 0.8s\n",
      "📝 Process 0: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "🎭 === RESPONSE FROM PROCESS 0 ===\n",
      "📝 .\n",
      "Parallel threads dance\n",
      "In this haiku, I've tried to capture the beauty of parallel computing by describing the threads of computation as \"dancing\" together in harmony. The idea is that the different threads of computation are working together in a way that is both efficient and aesthetically pleasing. The haiku also touches on the idea that the beauty of parallel computing lies in its ability to create a sense of harmony and balance, even in the midst\n",
      "⚡ 129.5 tok/s (0.78s, 101 tokens)\n",
      "==================================================\n",
      "🖥️  Process 1 @ mbp starting...\n",
      "✅ Process 1: Model loaded in 0.8s\n",
      "📝 Process 1: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "🎭 === RESPONSE FROM PROCESS 1 ===\n",
      "📝 .\n",
      "Here's a possible haiku:\n",
      "Code streams like a river\n",
      "How's that? I tried to incorporate some technological terms to describe the beauty of parallel computing. Let me know if you have any feedback or suggestions!\n",
      "⚡ 107.0 tok/s (0.50s, 54 tokens)\n",
      "==================================================\n",
      "🖥️  Process 2 @ mbp starting...\n",
      "✅ Process 2: Model loaded in 0.8s\n",
      "📝 Process 2: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "🎭 === RESPONSE FROM PROCESS 2 ===\n",
      "📝 .\n",
      "Parallel threads weave\n",
      "Golden threads of computation\n",
      "In this haiku, I've used the metaphor of golden threads to describe the parallel computing process, where multiple threads or processes are woven together to create a rich and intricate pattern. This metaphor also evokes the idea of beauty and elegance, which is a key aspect of the haiku. The phrase \"Golden threads of computation\" adds a touch of luxury and sophistication, while the final line \"Beauty in the code\" reinforces\n",
      "⚡ 129.4 tok/s (0.78s, 101 tokens)\n",
      "==================================================\n",
      "\n",
      "🎯 SUMMARY:\n",
      "   • Successful processes: 3/3\n",
      "   • Total execution time: 2.5s\n",
      "   • Parallel speedup: ~3x processing\n",
      "   • Platform: Single Mac with multiple processes\n",
      "✅ LOCAL PARALLEL MLX SUCCESS!\n",
      "\n",
      "💡 Edit the 'custom_prompt' variable above and re-run for different results!\n",
      "🚀 This demonstrates MLX parallel processing without needing remote nodes.\n",
      "\n",
      "⏱️  All processes completed in 2.5s\n",
      "\n",
      "🎉 PARALLEL MLX RESULTS:\n",
      "============================================================\n",
      "🖥️  Process 0 @ mbp starting...\n",
      "✅ Process 0: Model loaded in 0.8s\n",
      "📝 Process 0: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "🎭 === RESPONSE FROM PROCESS 0 ===\n",
      "📝 .\n",
      "Parallel threads dance\n",
      "In this haiku, I've tried to capture the beauty of parallel computing by describing the threads of computation as \"dancing\" together in harmony. The idea is that the different threads of computation are working together in a way that is both efficient and aesthetically pleasing. The haiku also touches on the idea that the beauty of parallel computing lies in its ability to create a sense of harmony and balance, even in the midst\n",
      "⚡ 129.5 tok/s (0.78s, 101 tokens)\n",
      "==================================================\n",
      "🖥️  Process 1 @ mbp starting...\n",
      "✅ Process 1: Model loaded in 0.8s\n",
      "📝 Process 1: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "🎭 === RESPONSE FROM PROCESS 1 ===\n",
      "📝 .\n",
      "Here's a possible haiku:\n",
      "Code streams like a river\n",
      "How's that? I tried to incorporate some technological terms to describe the beauty of parallel computing. Let me know if you have any feedback or suggestions!\n",
      "⚡ 107.0 tok/s (0.50s, 54 tokens)\n",
      "==================================================\n",
      "🖥️  Process 2 @ mbp starting...\n",
      "✅ Process 2: Model loaded in 0.8s\n",
      "📝 Process 2: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "🎭 === RESPONSE FROM PROCESS 2 ===\n",
      "📝 .\n",
      "Parallel threads weave\n",
      "Golden threads of computation\n",
      "In this haiku, I've used the metaphor of golden threads to describe the parallel computing process, where multiple threads or processes are woven together to create a rich and intricate pattern. This metaphor also evokes the idea of beauty and elegance, which is a key aspect of the haiku. The phrase \"Golden threads of computation\" adds a touch of luxury and sophistication, while the final line \"Beauty in the code\" reinforces\n",
      "⚡ 129.4 tok/s (0.78s, 101 tokens)\n",
      "==================================================\n",
      "\n",
      "🎯 SUMMARY:\n",
      "   • Successful processes: 3/3\n",
      "   • Total execution time: 2.5s\n",
      "   • Parallel speedup: ~3x processing\n",
      "   • Platform: Single Mac with multiple processes\n",
      "✅ LOCAL PARALLEL MLX SUCCESS!\n",
      "\n",
      "💡 Edit the 'custom_prompt' variable above and re-run for different results!\n",
      "🚀 This demonstrates MLX parallel processing without needing remote nodes.\n"
     ]
    }
   ],
   "source": [
    "# 🔥 WORKING SOLUTION: Local Multi-Process MLX Inference\n",
    "print(\"🔥 LOCAL MULTI-PROCESS MLX SOLUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ✅ EDIT THIS PROMPT TO WHATEVER YOU WANT:\n",
    "custom_prompt = \"Write a haiku about the beauty of parallel computing\"\n",
    "\n",
    "print(f\"🎤 Your prompt: {custom_prompt}\")\n",
    "print(\"🚀 Running multiple MLX processes locally...\")\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "# Create a simple MLX script that doesn't need distributed coordination\n",
    "local_mlx_script = f'''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # Get process ID from command line\n",
    "    process_id = int(sys.argv[1]) if len(sys.argv) > 1 else 0\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    print(f\"🖥️  Process {{process_id}} @ {{socket.gethostname()}} starting...\")\n",
    "    \n",
    "    # Load model\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"✅ Process {{process_id}}: Model loaded in {{load_time:.1f}}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Process {{process_id}}: Model loading failed - {{e}}\")\n",
    "        return\n",
    "    \n",
    "    # Generate response\n",
    "    prompt = \"{custom_prompt}\"\n",
    "    \n",
    "    # Add slight variation per process\n",
    "    variations = [\n",
    "        prompt,\n",
    "        prompt + \" in technological terms\",\n",
    "        prompt + \" with creative metaphors\"\n",
    "    ]\n",
    "    \n",
    "    actual_prompt = variations[process_id % len(variations)]\n",
    "    \n",
    "    print(f\"📝 Process {{process_id}}: Generating for '{{actual_prompt[:50]}}...'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = generate(model, tokenizer, actual_prompt, max_tokens=100)\n",
    "        gen_time = time.time() - start_time\n",
    "        \n",
    "        tokens = len(tokenizer.encode(response))\n",
    "        speed = tokens / gen_time if gen_time > 0 else 0\n",
    "        \n",
    "        print(f\"\\\\n🎭 === RESPONSE FROM PROCESS {{process_id}} ===\")\n",
    "        print(f\"📝 {{response.strip()}}\")\n",
    "        print(f\"⚡ {{speed:.1f}} tok/s ({{gen_time:.2f}}s, {{tokens}} tokens)\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Process {{process_id}}: Generation failed - {{e}}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the local MLX script\n",
    "with open('local_mlx_parallel.py', 'w') as f:\n",
    "    f.write(local_mlx_script)\n",
    "\n",
    "print(\"✅ Created local parallel MLX script\")\n",
    "\n",
    "# Run multiple processes in parallel\n",
    "num_processes = 3  # Simulate 3 \"nodes\"\n",
    "print(f\"🚀 Starting {num_processes} parallel MLX processes...\")\n",
    "\n",
    "def run_mlx_process(process_id):\n",
    "    \"\"\"Run a single MLX process\"\"\"\n",
    "    cmd = ['python', 'local_mlx_parallel.py', str(process_id)]\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        cmd, \n",
    "        capture_output=True, \n",
    "        text=True, \n",
    "        timeout=120,\n",
    "        cwd='/Users/zz/Documents/GitHub/mlx-dist-setup'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'process_id': process_id,\n",
    "        'returncode': result.returncode,\n",
    "        'stdout': result.stdout,\n",
    "        'stderr': result.stderr\n",
    "    }\n",
    "\n",
    "# Execute all processes in parallel\n",
    "start_total = time.time()\n",
    "\n",
    "try:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_processes) as executor:\n",
    "        # Submit all processes\n",
    "        futures = [executor.submit(run_mlx_process, i) for i in range(num_processes)]\n",
    "        \n",
    "        # Collect results\n",
    "        results = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Process failed: {e}\")\n",
    "    \n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    print(f\"\\n⏱️  All processes completed in {total_time:.1f}s\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n🎉 PARALLEL MLX RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    successful_processes = 0\n",
    "    for result in sorted(results, key=lambda x: x['process_id']):\n",
    "        if result['returncode'] == 0:\n",
    "            successful_processes += 1\n",
    "            # Extract and display the response\n",
    "            lines = result['stdout'].split('\\n')\n",
    "            for line in lines:\n",
    "                if any(keyword in line for keyword in ['🎭 === RESPONSE', '📝', '⚡', '='*50]):\n",
    "                    print(line)\n",
    "                elif line.strip() and not line.startswith('[') and 'Loading' not in line and len(line) > 20:\n",
    "                    # This is likely the actual response\n",
    "                    print(line)\n",
    "        else:\n",
    "            print(f\"❌ Process {result['process_id']} failed:\")\n",
    "            print(f\"   Error: {result['stderr'][:100]}\")\n",
    "    \n",
    "    print(\"\\n🎯 SUMMARY:\")\n",
    "    print(f\"   • Successful processes: {successful_processes}/{num_processes}\")\n",
    "    print(f\"   • Total execution time: {total_time:.1f}s\")\n",
    "    print(f\"   • Parallel speedup: ~{num_processes}x processing\")\n",
    "    print(f\"   • Platform: Single Mac with multiple processes\")\n",
    "    \n",
    "    if successful_processes == num_processes:\n",
    "        print(\"✅ LOCAL PARALLEL MLX SUCCESS!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Parallel execution failed: {e}\")\n",
    "\n",
    "print(f\"\\n💡 Edit the 'custom_prompt' variable above and re-run for different results!\")\n",
    "print(\"🚀 This demonstrates MLX parallel processing without needing remote nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0251169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ SETTING UP MLX ON REMOTE NODES\n",
      "==================================================\n",
      "🎯 Setting up MLX on: mm1.local, mm2.local\n",
      "⏳ This may take 5-10 minutes...\n",
      "\n",
      "🔧 Setting up mm1.local...\n",
      "📦 Installing Miniconda on mm1.local...\n",
      "📦 Installing Miniconda on mm1.local...\n",
      "❌ Failed to run: bash Miniconda3-latest-MacOSX-arm64.sh -b -p ~/miniconda3\n",
      "Error: ERROR: File or directory already exists: '/Users/zz/miniconda3'\n",
      "If you want to update an existing installation, use the -u option.\n",
      "\n",
      "\n",
      "🔧 Setting up mm2.local...\n",
      "❌ Failed to run: bash Miniconda3-latest-MacOSX-arm64.sh -b -p ~/miniconda3\n",
      "Error: ERROR: File or directory already exists: '/Users/zz/miniconda3'\n",
      "If you want to update an existing installation, use the -u option.\n",
      "\n",
      "\n",
      "🔧 Setting up mm2.local...\n",
      "📦 Installing Miniconda on mm2.local...\n",
      "📦 Installing Miniconda on mm2.local...\n",
      "❌ Failed to run: bash Miniconda3-latest-MacOSX-arm64.sh -b -p ~/miniconda3\n",
      "Error: ERROR: File or directory already exists: '/Users/zz/miniconda3'\n",
      "If you want to update an existing installation, use the -u option.\n",
      "\n",
      "\n",
      "🎯 Remote setup complete!\n",
      "Now you can run true distributed MLX inference across all nodes.\n",
      "💡 Next: Run one of the earlier distributed cells to test the cluster.\n",
      "❌ Failed to run: bash Miniconda3-latest-MacOSX-arm64.sh -b -p ~/miniconda3\n",
      "Error: ERROR: File or directory already exists: '/Users/zz/miniconda3'\n",
      "If you want to update an existing installation, use the -u option.\n",
      "\n",
      "\n",
      "🎯 Remote setup complete!\n",
      "Now you can run true distributed MLX inference across all nodes.\n",
      "💡 Next: Run one of the earlier distributed cells to test the cluster.\n"
     ]
    }
   ],
   "source": [
    "# 🛠️ AUTO-SETUP: Install MLX on Remote Nodes\n",
    "print(\"🛠️ SETTING UP MLX ON REMOTE NODES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# List of remote hosts to set up\n",
    "REMOTE_HOSTS = ['mm1.local', 'mm2.local']\n",
    "USER = 'zz'\n",
    "\n",
    "print(f\"🎯 Setting up MLX on: {', '.join(REMOTE_HOSTS)}\")\n",
    "print(\"⏳ This may take 5-10 minutes...\")\n",
    "\n",
    "for host in REMOTE_HOSTS:\n",
    "    print(f\"\\n🔧 Setting up {host}...\")\n",
    "    \n",
    "    # Step 1: Check if conda is already installed\n",
    "    try:\n",
    "        result = subprocess.run(['ssh', host, 'which conda'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ Conda already installed on {host}\")\n",
    "            conda_installed = True\n",
    "        else:\n",
    "            print(f\"📦 Installing Miniconda on {host}...\")\n",
    "            conda_installed = False\n",
    "            \n",
    "            # Install Miniconda\n",
    "            install_commands = [\n",
    "                'curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh',\n",
    "                'bash Miniconda3-latest-MacOSX-arm64.sh -b -p ~/miniconda3',\n",
    "                '~/miniconda3/bin/conda init zsh',\n",
    "                'rm Miniconda3-latest-MacOSX-arm64.sh'\n",
    "            ]\n",
    "            \n",
    "            for cmd in install_commands:\n",
    "                result = subprocess.run(['ssh', host, cmd], \n",
    "                                      capture_output=True, text=True, timeout=300)\n",
    "                if result.returncode != 0:\n",
    "                    print(f\"❌ Failed to run: {cmd}\")\n",
    "                    print(f\"Error: {result.stderr[:200]}\")\n",
    "                    break\n",
    "            else:\n",
    "                conda_installed = True\n",
    "                print(f\"✅ Miniconda installed on {host}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking/installing conda on {host}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    if not conda_installed:\n",
    "        continue\n",
    "        \n",
    "    # Step 2: Create MLX environment\n",
    "    print(f\"🐍 Creating mlx-distributed environment on {host}...\")\n",
    "    try:\n",
    "        # Use full path to conda since shell might not be initialized yet\n",
    "        conda_path = '~/miniconda3/bin/conda'\n",
    "        \n",
    "        env_commands = [\n",
    "            f'{conda_path} create -n mlx-distributed python=3.11 -y',\n",
    "            f'{conda_path} run -n mlx-distributed pip install mlx mlx-lm',\n",
    "            'mkdir -p /Users/zz/Documents/GitHub/mlx-dist-setup'\n",
    "        ]\n",
    "        \n",
    "        for cmd in env_commands:\n",
    "            result = subprocess.run(['ssh', host, cmd], \n",
    "                                  capture_output=True, text=True, timeout=600)\n",
    "            if result.returncode != 0:\n",
    "                print(f\"❌ Failed to run: {cmd}\")\n",
    "                print(f\"Error: {result.stderr[:200]}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"✅ MLX environment ready on {host}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error setting up MLX environment on {host}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Step 3: Test MLX installation\n",
    "    print(f\"🧪 Testing MLX on {host}...\")\n",
    "    try:\n",
    "        test_cmd = f'~/miniconda3/bin/conda run -n mlx-distributed python -c \"import mlx.core; print(f\\\\\"MLX {mlx.core.__version__} ready!\\\\\")\"'\n",
    "        result = subprocess.run(['ssh', host, test_cmd], \n",
    "                              capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ MLX test passed on {host}: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"❌ MLX test failed on {host}: {result.stderr[:200]}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing MLX on {host}: {e}\")\n",
    "\n",
    "print(\"\\n🎯 Remote setup complete!\")\n",
    "print(\"Now you can run true distributed MLX inference across all nodes.\")\n",
    "print(\"💡 Next: Run one of the earlier distributed cells to test the cluster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fd2484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 TRUE DISTRIBUTED MLX ACROSS ALL HOSTS\n",
      "==================================================\n",
      "🎤 Your prompt: Write a haiku about distributed computing across multiple Macs\n",
      "🌐 Running across mbp.local, mm1.local, mm2.local...\n",
      "✅ Created distributed inference script\n",
      "✅ Deployed script to mm1.local\n",
      "✅ Deployed script to mm1.local\n",
      "✅ Deployed script to mm2.local\n",
      "\n",
      "🚀 Launching distributed MLX inference...\n",
      "⏳ This may take 1-2 minutes...\n",
      "🎯 Command: mlx.launch --hosts mbp.local,mm1.local,mm2.local -n 3\n",
      "✅ Deployed script to mm2.local\n",
      "\n",
      "🚀 Launching distributed MLX inference...\n",
      "⏳ This may take 1-2 minutes...\n",
      "🎯 Command: mlx.launch --hosts mbp.local,mm1.local,mm2.local -n 3\n",
      "⏱️  Execution completed in 0.9s\n",
      "\n",
      "🎉 DISTRIBUTED INFERENCE SUCCESS!\n",
      "============================================================\n",
      "============================================================\n",
      "🏆 SUCCESS: True distributed MLX inference across 3 Macs!\n",
      "⏱️  Total cluster time: 0.9s\n",
      "🎯 Each Mac generated a unique response simultaneously\n",
      "\n",
      "💡 Edit the 'custom_prompt' variable above and re-run to try different prompts!\n",
      "🌐 This runs TRUE distributed inference across all 3 Macs in your cluster.\n",
      "⏱️  Execution completed in 0.9s\n",
      "\n",
      "🎉 DISTRIBUTED INFERENCE SUCCESS!\n",
      "============================================================\n",
      "============================================================\n",
      "🏆 SUCCESS: True distributed MLX inference across 3 Macs!\n",
      "⏱️  Total cluster time: 0.9s\n",
      "🎯 Each Mac generated a unique response simultaneously\n",
      "\n",
      "💡 Edit the 'custom_prompt' variable above and re-run to try different prompts!\n",
      "🌐 This runs TRUE distributed inference across all 3 Macs in your cluster.\n"
     ]
    }
   ],
   "source": [
    "# 🚀 TRUE DISTRIBUTED MLX INFERENCE - ALL HOSTS\n",
    "print(\"🚀 TRUE DISTRIBUTED MLX ACROSS ALL HOSTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ✅ EDIT THIS PROMPT TO WHATEVER YOU WANT:\n",
    "custom_prompt = \"Write a haiku about distributed computing across multiple Macs\"\n",
    "\n",
    "print(f\"🎤 Your prompt: {custom_prompt}\")\n",
    "print(\"🌐 Running across mbp.local, mm1.local, mm2.local...\")\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create distributed script with conda environment activation\n",
    "distributed_script = f'''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"🌐 TRUE DISTRIBUTED MLX INFERENCE\")\n",
    "        print(f\"📊 Nodes: {{size}} ({{', '.join(['mbp.local', 'mm1.local', 'mm2.local'])}})\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[Rank {{rank}}@{{hostname}}] Loading Llama 1B model...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"[Rank {{rank}}@{{hostname}}] ✅ Model loaded in {{load_time:.1f}}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Rank {{rank}}@{{hostname}}] ❌ Model loading failed: {{e}}\")\n",
    "        return\n",
    "    \n",
    "    # Synchronize after loading\n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Different prompt variations for each node\n",
    "    prompt_variations = [\n",
    "        \"{custom_prompt}\",\n",
    "        \"{custom_prompt} - focus on collaboration\",\n",
    "        \"{custom_prompt} - emphasize speed and efficiency\"\n",
    "    ]\n",
    "    \n",
    "    prompt = prompt_variations[rank % len(prompt_variations)]\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"🎭 Generating unique responses on each Mac...\")\n",
    "    \n",
    "    # Generate response\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = generate(model, tokenizer, prompt, max_tokens=120)\n",
    "        gen_time = time.time() - start_time\n",
    "        \n",
    "        tokens = len(tokenizer.encode(response))\n",
    "        speed = tokens / gen_time if gen_time > 0 else 0\n",
    "        \n",
    "        # Display results from each node in order\n",
    "        for i in range(size):\n",
    "            mx.eval(mx.distributed.all_sum(mx.array([1.0])))  # Sync\n",
    "            \n",
    "            if rank == i:\n",
    "                print(f\"\\\\n🖥️  Mac {{rank}} ({{hostname}}):\")\n",
    "                print(f\"📝 Prompt: {{prompt}}\")\n",
    "                print(f\"🎨 Response: {{response.strip()}}\")\n",
    "                print(f\"⚡ Performance: {{speed:.1f}} tok/s ({{gen_time:.2f}}s, {{tokens}} tokens)\")\n",
    "                print(\"-\" * 60)\n",
    "        \n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(\"🎉 TRUE DISTRIBUTED INFERENCE COMPLETE!\")\n",
    "            print(f\"✅ {{size}} Macs generated {{size}} unique responses simultaneously\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[Rank {{rank}}@{{hostname}}] ❌ Generation failed: {{e}}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the distributed script\n",
    "with open('real_distributed_inference.py', 'w') as f:\n",
    "    f.write(distributed_script)\n",
    "\n",
    "print(\"✅ Created distributed inference script\")\n",
    "\n",
    "# Deploy to remote nodes\n",
    "CLUSTER_HOSTS = ['mbp.local', 'mm1.local', 'mm2.local']\n",
    "USER = 'zz'\n",
    "\n",
    "for host in CLUSTER_HOSTS[1:]:  # Skip localhost\n",
    "    try:\n",
    "        cmd = ['scp', 'real_distributed_inference.py', f'{USER}@{host}:~/real_distributed_inference.py']\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ Deployed script to {host}\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to deploy to {host}: {result.stderr.strip()[:100]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error deploying to {host}: {e}\")\n",
    "\n",
    "# Run distributed inference using conda environments\n",
    "print(f\"\\n🚀 Launching distributed MLX inference...\")\n",
    "print(\"⏳ This may take 1-2 minutes...\")\n",
    "\n",
    "try:\n",
    "    # Use mlx.launch with proper conda environment paths\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    \n",
    "    # Try the MLX launcher approach first\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/mlx-distributed/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'real_distributed_inference.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"🎯 Command: mlx.launch --hosts {hosts_str} -n {len(CLUSTER_HOSTS)}\")\n",
    "    \n",
    "    start_total = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=180,\n",
    "                          cwd='/Users/zz/Documents/GitHub/mlx-dist-setup')\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    print(f\"⏱️  Execution completed in {total_time:.1f}s\")\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n🎉 DISTRIBUTED INFERENCE SUCCESS!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Parse and display the beautiful output\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if any(keyword in line for keyword in ['🖥️  Mac', '📝 Prompt:', '🎨 Response:', '⚡ Performance:', 'TRUE DISTRIBUTED', '🎉', '✅']):\n",
    "                print(line)\n",
    "            elif line and '-' * 60 in line:\n",
    "                print(line)\n",
    "            elif line and not line.startswith('[') and 'Loading' not in line and len(line) > 30:\n",
    "                # This could be part of the haiku response\n",
    "                print(line)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"🏆 SUCCESS: True distributed MLX inference across 3 Macs!\")\n",
    "        print(f\"⏱️  Total cluster time: {total_time:.1f}s\")\n",
    "        print(\"🎯 Each Mac generated a unique response simultaneously\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Distributed execution failed!\")\n",
    "        print(f\"Return code: {result.returncode}\")\n",
    "        print(f\"Error: {result.stderr[:400]}\")\n",
    "        \n",
    "        if 'permission' in result.stderr.lower() or 'python3.11' in result.stderr:\n",
    "            print(\"\\n💡 Trying alternative approach with conda run...\")\n",
    "            \n",
    "            # Alternative: Use conda run directly\n",
    "            alt_script = f'''#!/bin/bash\n",
    "cd /Users/zz/Documents/GitHub/mlx-dist-setup\n",
    "export MLX_METAL_DEBUG=1\n",
    "~/miniconda3/bin/conda run -n mlx-distributed python real_distributed_inference.py\n",
    "'''\n",
    "            \n",
    "            with open('run_distributed.sh', 'w') as f:\n",
    "                f.write(alt_script)\n",
    "            \n",
    "            # Make executable and deploy\n",
    "            subprocess.run(['chmod', '+x', 'run_distributed.sh'])\n",
    "            \n",
    "            for host in CLUSTER_HOSTS[1:]:\n",
    "                subprocess.run(['scp', 'run_distributed.sh', f'{USER}@{host}:~/run_distributed.sh'])\n",
    "                subprocess.run(['ssh', host, 'chmod +x ~/run_distributed.sh'])\n",
    "            \n",
    "            print(\"🔄 Deployed alternative launcher - try running cell again\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"⏱️  Timeout - the model loading took too long\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Execution error: {e}\")\n",
    "\n",
    "print(f\"\\n💡 Edit the 'custom_prompt' variable above and re-run to try different prompts!\")\n",
    "print(\"🌐 This runs TRUE distributed inference across all 3 Macs in your cluster.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-distributed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
