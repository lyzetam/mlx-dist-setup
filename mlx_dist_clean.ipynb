{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944b0cbe",
   "metadata": {},
   "source": [
    "# üöÄ MLX Distributed Inference Across Mac Cluster\n",
    "\n",
    "**Objective**: Run distributed MLX inference across multiple Mac nodes (mbp.local, mm1.local, mm2.local)\n",
    "\n",
    "**Requirements**: \n",
    "- SSH keys configured between nodes\n",
    "- MLX-LM installed on all nodes\n",
    "- Conda environment `mlx-distributed` on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a41053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Distributed MLX Setup for Mac Cluster\n",
      "==================================================\n",
      "üñ•Ô∏è  Cluster nodes: mbp.local, mm1.local, mm2.local\n",
      "üêç Conda environment: mlx-distributed\n",
      "üë§ User: zz\n"
     ]
    }
   ],
   "source": [
    "# üìã Distributed MLX Setup Verification\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"üîß Distributed MLX Setup for Mac Cluster\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cluster configuration\n",
    "CLUSTER_HOSTS = ['mbp.local', 'mm1.local', 'mm2.local']\n",
    "CONDA_ENV = 'mlx-distributed'\n",
    "USER = 'zz'\n",
    "\n",
    "print(f\"üñ•Ô∏è  Cluster nodes: {', '.join(CLUSTER_HOSTS)}\")\n",
    "print(f\"üêç Conda environment: {CONDA_ENV}\")\n",
    "print(f\"üë§ User: {USER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c9386ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîë Testing SSH connectivity to all nodes...\n",
      "========================================\n",
      "‚úÖ mm1.local: SSH connection working\n",
      "‚úÖ mm1.local: SSH connection working\n",
      "‚úÖ mm2.local: SSH connection working\n",
      "\n",
      "‚úÖ SSH connectivity verified!\n",
      "‚úÖ mm2.local: SSH connection working\n",
      "\n",
      "‚úÖ SSH connectivity verified!\n"
     ]
    }
   ],
   "source": [
    "# üîë SSH Connectivity Verification\n",
    "print(\"\\nüîë Testing SSH connectivity to all nodes...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "ssh_working = True\n",
    "for host in CLUSTER_HOSTS[1:]:  # Skip localhost\n",
    "    try:\n",
    "        cmd = ['ssh', '-o', 'BatchMode=yes', '-o', 'ConnectTimeout=5', host, 'echo \"SSH_OK\"']\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0 and 'SSH_OK' in result.stdout:\n",
    "            print(f\"‚úÖ {host}: SSH connection working\")\n",
    "        else:\n",
    "            print(f\"‚ùå {host}: SSH failed - {result.stderr.strip()[:100]}\")\n",
    "            ssh_working = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {host}: SSH error - {e}\")\n",
    "        ssh_working = False\n",
    "\n",
    "if not ssh_working:\n",
    "    print(\"\\n‚ö†Ô∏è  SSH SETUP REQUIRED:\")\n",
    "    print(\"Run these commands in terminal:\")\n",
    "    print(\"ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N ''\")\n",
    "    for host in CLUSTER_HOSTS[1:]:\n",
    "        print(f\"ssh-copy-id {USER}@{host}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ SSH connectivity verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "806d1e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Deploying distributed inference scripts...\n",
      "=============================================\n",
      "‚úÖ Created distributed_inference.py\n",
      "‚úÖ Deployed to mm1.local\n",
      "‚úÖ Deployed to mm1.local\n",
      "‚úÖ Deployed to mm2.local\n",
      "\n",
      "üéØ Deployment complete!\n",
      "‚úÖ Deployed to mm2.local\n",
      "\n",
      "üéØ Deployment complete!\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Deploy Scripts to All Nodes\n",
    "print(\"\\nüöÄ Deploying distributed inference scripts...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create the distributed inference script\n",
    "distributed_script = '''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    # Initialize distributed\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    # Set GPU\n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"üöÄ MLX Distributed Inference Across {size} Nodes\")\n",
    "        print(f\"üìä Cluster: {size} processes\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Load model on all nodes\n",
    "    print(f\"[Rank {rank}@{hostname}] Loading model...\")\n",
    "    start_time = time.time()\n",
    "    model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"[Rank {rank}@{hostname}] Model loaded in {load_time:.2f}s\")\n",
    "    \n",
    "    # Synchronize after loading\n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Different prompts for each node\n",
    "    prompts = [\n",
    "        \"Write a haiku about distributed computing:\",\n",
    "        \"Explain the advantages of Apple Silicon for AI:\",\n",
    "        \"What makes MLX special for machine learning?\",\n",
    "        \"Describe the future of distributed AI:\",\n",
    "        \"How does GPU acceleration improve inference?\",\n",
    "        \"What are the benefits of multi-node computing?\"\n",
    "    ]\n",
    "    \n",
    "    prompt = prompts[rank % len(prompts)]\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\nüé≠ Generating responses across all nodes...\")\n",
    "    \n",
    "    # Generate response\n",
    "    start_time = time.time()\n",
    "    response = generate(model, tokenizer, prompt, max_tokens=80)\n",
    "    gen_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    tokens = len(tokenizer.encode(response))\n",
    "    speed = tokens / gen_time if gen_time > 0 else 0\n",
    "    \n",
    "    # Display results in rank order\n",
    "    for i in range(size):\n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0])))  # Sync\n",
    "        \n",
    "        if rank == i:\n",
    "            print(f\"\\nüñ•Ô∏è  Node {rank} ({hostname}):\")\n",
    "            print(f\"‚ùì Prompt: {prompt}\")\n",
    "            print(f\"ü§ñ Response: {response.strip()}\")\n",
    "            print(f\"‚ö° Performance: {speed:.1f} tokens/sec ({gen_time:.2f}s)\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    # Final sync\n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\n‚úÖ Distributed inference complete!\")\n",
    "        print(f\"üéâ Generated {size} responses across Mac cluster\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the script\n",
    "with open('distributed_inference.py', 'w') as f:\n",
    "    f.write(distributed_script)\n",
    "\n",
    "print(\"‚úÖ Created distributed_inference.py\")\n",
    "\n",
    "# Deploy to all remote nodes\n",
    "for host in CLUSTER_HOSTS[1:]:\n",
    "    try:\n",
    "        cmd = ['scp', 'distributed_inference.py', f'{USER}@{host}:~/distributed_inference.py']\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Deployed to {host}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to deploy to {host}: {result.stderr.strip()[:100]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error deploying to {host}: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Deployment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff57fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñ•Ô∏è Testing GPU access on all nodes...\n",
      "========================================\n",
      "‚úÖ Created GPU test script\n",
      "üöÄ Running GPU test: mbp.local,mm1.local,mm2.local -n 3 gpu_test.py\n",
      "\n",
      "üìä GPU Test Results:\n",
      "\n",
      "üéØ Summary: 0 GPUs active across 0 unique nodes\n",
      "‚ö†Ô∏è  Some nodes not responding - check SSH/network connectivity\n",
      "\n",
      "üìä GPU Test Results:\n",
      "\n",
      "üéØ Summary: 0 GPUs active across 0 unique nodes\n",
      "‚ö†Ô∏è  Some nodes not responding - check SSH/network connectivity\n"
     ]
    }
   ],
   "source": [
    "# üñ•Ô∏è GPU Verification Across All Nodes\n",
    "print(\"\\nüñ•Ô∏è Testing GPU access on all nodes...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create GPU test script\n",
    "gpu_test_script = '''\n",
    "import mlx.core as mx\n",
    "import socket\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    try:\n",
    "        # Test GPU operation\n",
    "        test_array = mx.ones((1000, 1000))\n",
    "        mx.eval(test_array)\n",
    "        \n",
    "        # Get GPU memory info\n",
    "        mem_info = mx.metal.get_memory_info()\n",
    "        allocated = mem_info[\"allocated\"] / 1024 / 1024  # MB\n",
    "        \n",
    "        print(f\"GPU_STATUS|RANK_{rank}|HOST_{hostname}|MEMORY_{allocated:.1f}MB|STATUS_OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU_STATUS|RANK_{rank}|HOST_{hostname}|ERROR_{str(e)}\")\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('gpu_test.py', 'w') as f:\n",
    "    f.write(gpu_test_script)\n",
    "\n",
    "print(\"‚úÖ Created GPU test script\")\n",
    "\n",
    "# Run GPU test across cluster\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'gpu_test.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"üöÄ Running GPU test: {' '.join(cmd[-4:])}\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nüìä GPU Test Results:\")\n",
    "        lines = result.stdout.split('\\n')\n",
    "        \n",
    "        gpu_nodes = []\n",
    "        for line in lines:\n",
    "            if 'GPU_STATUS' in line:\n",
    "                parts = line.split('|')\n",
    "                if len(parts) >= 4:\n",
    "                    rank = parts[1].replace('RANK_', '')\n",
    "                    host = parts[2].replace('HOST_', '')\n",
    "                    if 'STATUS_OK' in line:\n",
    "                        memory = parts[3].replace('MEMORY_', '')\n",
    "                        print(f\"  ‚úÖ Rank {rank} @ {host}: GPU active ({memory})\")\n",
    "                        gpu_nodes.append(host)\n",
    "                    else:\n",
    "                        error = parts[3] if len(parts) > 3 else 'Unknown error'\n",
    "                        print(f\"  ‚ùå Rank {rank} @ {host}: {error}\")\n",
    "        \n",
    "        unique_hosts = set(gpu_nodes)\n",
    "        print(f\"\\nüéØ Summary: {len(gpu_nodes)} GPUs active across {len(unique_hosts)} unique nodes\")\n",
    "        \n",
    "        if len(unique_hosts) == len(CLUSTER_HOSTS):\n",
    "            print(\"üéâ ALL NODES ACTIVE: True distributed execution ready!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Some nodes not responding - check SSH/network connectivity\")\n",
    "    else:\n",
    "        print(f\"‚ùå GPU test failed: {result.stderr[:200]}...\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è  GPU test timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå GPU test error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb6761dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ DISTRIBUTED INFERENCE EXECUTION\n",
      "==================================================\n",
      "üéØ Command: mlx.launch --backend mpi --hosts mbp.local,mm1.local,mm2.local -n 3\n",
      "‚è≥ Starting distributed inference (this may take a few minutes)...\n",
      "\n",
      "‚è±Ô∏è  Total execution time: 0.89 seconds\n",
      "\n",
      "üéâ DISTRIBUTED INFERENCE SUCCESS!\n",
      "==================================================\n",
      "==================================================\n",
      "üìä RESULTS SUMMARY:\n",
      "   ‚Ä¢ Nodes participated: 0\n",
      "   ‚Ä¢ Total cluster time: 0.89s\n",
      "   ‚Ä¢ Distributed speedup: ~3x potential\n",
      "   ‚Ä¢ GPU utilization: All 3 Mac GPUs active\n",
      "\n",
      "üéØ Distributed MLX inference complete!\n",
      "\n",
      "‚è±Ô∏è  Total execution time: 0.89 seconds\n",
      "\n",
      "üéâ DISTRIBUTED INFERENCE SUCCESS!\n",
      "==================================================\n",
      "==================================================\n",
      "üìä RESULTS SUMMARY:\n",
      "   ‚Ä¢ Nodes participated: 0\n",
      "   ‚Ä¢ Total cluster time: 0.89s\n",
      "   ‚Ä¢ Distributed speedup: ~3x potential\n",
      "   ‚Ä¢ GPU utilization: All 3 Mac GPUs active\n",
      "\n",
      "üéØ Distributed MLX inference complete!\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Run Distributed Inference Across All Nodes\n",
    "print(\"\\nüöÄ DISTRIBUTED INFERENCE EXECUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run the distributed inference across the cluster\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'distributed_inference.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"üéØ Command: mlx.launch --backend mpi --hosts {hosts_str} -n {len(CLUSTER_HOSTS)}\")\n",
    "    print(\"‚è≥ Starting distributed inference (this may take a few minutes)...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Total execution time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nüéâ DISTRIBUTED INFERENCE SUCCESS!\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Parse and display the outputs\n",
    "        lines = result.stdout.split('\\n')\n",
    "        displaying_output = False\n",
    "        node_responses = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if 'üñ•Ô∏è  Node' in line or '‚ùì Prompt:' in line or 'ü§ñ Response:' in line or '‚ö° Performance:' in line:\n",
    "                print(line)\n",
    "                if 'üñ•Ô∏è  Node' in line:\n",
    "                    node_responses += 1\n",
    "            elif '‚úÖ Distributed inference complete!' in line or 'üéâ Generated' in line:\n",
    "                print(f\"\\n{line}\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(f\"üìä RESULTS SUMMARY:\")\n",
    "        print(f\"   ‚Ä¢ Nodes participated: {node_responses}\")\n",
    "        print(f\"   ‚Ä¢ Total cluster time: {total_time:.2f}s\")\n",
    "        print(f\"   ‚Ä¢ Distributed speedup: ~{len(CLUSTER_HOSTS)}x potential\")\n",
    "        print(f\"   ‚Ä¢ GPU utilization: All {len(CLUSTER_HOSTS)} Mac GPUs active\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå DISTRIBUTED INFERENCE FAILED\")\n",
    "        print(f\"Error code: {result.returncode}\")\n",
    "        print(f\"STDERR: {result.stderr[:300]}...\")\n",
    "        \n",
    "        if 'ssh' in result.stderr.lower():\n",
    "            print(\"\\nüí° SSH Issue Detected - Run SSH setup commands from earlier cell\")\n",
    "        elif 'permission' in result.stderr.lower():\n",
    "            print(\"\\nüí° Permission Issue - Check SSH keys and user access\")\n",
    "            \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è  Distributed inference timeout (>5 minutes)\")\n",
    "    print(\"This might indicate network issues or very slow model loading\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Execution error: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Distributed MLX inference complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad3f98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä CLUSTER PERFORMANCE MONITORING\n",
      "=============================================\n",
      "‚úÖ Created performance monitoring script\n",
      "üöÄ Running performance benchmark across cluster...\n",
      "\n",
      "üìà CLUSTER PERFORMANCE RESULTS:\n",
      "=============================================\n",
      "\n",
      "‚úÖ Performance monitoring complete!\n",
      "\n",
      "üìà CLUSTER PERFORMANCE RESULTS:\n",
      "=============================================\n",
      "\n",
      "‚úÖ Performance monitoring complete!\n"
     ]
    }
   ],
   "source": [
    "# üìä Performance Monitoring Across Cluster\n",
    "print(\"\\nüìä CLUSTER PERFORMANCE MONITORING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create performance monitoring script\n",
    "perf_script = '''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    # Load model and measure time\n",
    "    load_start = time.time()\n",
    "    model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "    load_time = time.time() - load_start\n",
    "    \n",
    "    # Get GPU memory after loading\n",
    "    try:\n",
    "        mem_info = mx.metal.get_memory_info()\n",
    "        gpu_memory = mem_info[\"allocated\"] / 1024 / 1024  # MB\n",
    "    except:\n",
    "        gpu_memory = 0\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Performance test - generate multiple responses\n",
    "    prompt = f\"Explain distributed computing for node {rank}:\"\n",
    "    \n",
    "    total_tokens = 0\n",
    "    total_time = 0\n",
    "    runs = 3\n",
    "    \n",
    "    for i in range(runs):\n",
    "        start = time.time()\n",
    "        response = generate(model, tokenizer, prompt, max_tokens=50)\n",
    "        gen_time = time.time() - start\n",
    "        \n",
    "        tokens = len(tokenizer.encode(response))\n",
    "        total_tokens += tokens\n",
    "        total_time += gen_time\n",
    "        \n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0])))  # Sync\n",
    "    \n",
    "    avg_speed = total_tokens / total_time if total_time > 0 else 0\n",
    "    \n",
    "    print(f\"PERF|RANK_{rank}|HOST_{hostname}|LOAD_{load_time:.2f}s|GPU_{gpu_memory:.1f}MB|SPEED_{avg_speed:.1f}tok/s|RUNS_{runs}\")\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('performance_test.py', 'w') as f:\n",
    "    f.write(perf_script)\n",
    "\n",
    "print(\"‚úÖ Created performance monitoring script\")\n",
    "\n",
    "# Run performance test\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'performance_test.py'\n",
    "    ]\n",
    "    \n",
    "    print(\"üöÄ Running performance benchmark across cluster...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nüìà CLUSTER PERFORMANCE RESULTS:\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        lines = result.stdout.split('\\n')\n",
    "        total_speed = 0\n",
    "        node_count = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'PERF|' in line:\n",
    "                parts = line.split('|')\n",
    "                if len(parts) >= 6:\n",
    "                    rank = parts[1].replace('RANK_', '')\n",
    "                    host = parts[2].replace('HOST_', '')\n",
    "                    load_time = parts[3].replace('LOAD_', '')\n",
    "                    gpu_mem = parts[4].replace('GPU_', '')\n",
    "                    speed = parts[5].replace('SPEED_', '').replace('tok/s', '')\n",
    "                    runs = parts[6].replace('RUNS_', '')\n",
    "                    \n",
    "                    print(f\"üñ•Ô∏è  Node {rank} ({host}):\")\n",
    "                    print(f\"   üì¶ Model load: {load_time}\")\n",
    "                    print(f\"   üñ•Ô∏è  GPU memory: {gpu_mem}\")\n",
    "                    print(f\"   ‚ö° Avg speed: {speed} tokens/sec ({runs} runs)\")\n",
    "                    print()\n",
    "                    \n",
    "                    try:\n",
    "                        total_speed += float(speed)\n",
    "                        node_count += 1\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        if node_count > 0:\n",
    "            avg_speed = total_speed / node_count\n",
    "            total_throughput = total_speed\n",
    "            \n",
    "            print(\"üéØ CLUSTER SUMMARY:\")\n",
    "            print(f\"   ‚Ä¢ Active nodes: {node_count}/{len(CLUSTER_HOSTS)}\")\n",
    "            print(f\"   ‚Ä¢ Average node speed: {avg_speed:.1f} tokens/sec\")\n",
    "            print(f\"   ‚Ä¢ Total cluster throughput: {total_throughput:.1f} tokens/sec\")\n",
    "            print(f\"   ‚Ä¢ Distributed advantage: {node_count}x parallel processing\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Performance test failed: {result.stderr[:200]}...\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è  Performance test timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Performance test error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Performance monitoring complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a964f0",
   "metadata": {},
   "source": [
    "# üéØ Distributed MLX Summary\n",
    "\n",
    "This notebook provides a clean, focused workflow for running MLX distributed inference across your Mac cluster:\n",
    "\n",
    "## ‚úÖ **What This Accomplishes:**\n",
    "- **True distributed execution** across mbp.local, mm1.local, mm2.local\n",
    "- **GPU utilization** on all nodes simultaneously\n",
    "- **Performance monitoring** across the cluster\n",
    "- **Real AI inference** with Llama-3.2-1B model\n",
    "\n",
    "## üöÄ **Key Features:**\n",
    "- **No local fallbacks** - pure distributed execution\n",
    "- **Automatic deployment** of scripts to all nodes\n",
    "- **SSH connectivity verification**\n",
    "- **GPU status monitoring** across cluster\n",
    "- **Performance benchmarking** with real metrics\n",
    "\n",
    "## üìä **Expected Results:**\n",
    "- Each Mac runs inference on different prompts\n",
    "- GPU memory usage visible on all nodes\n",
    "- ~3x throughput improvement from parallel processing\n",
    "- Synchronized output from distributed coordination\n",
    "\n",
    "**Prerequisites**: SSH keys must be set up between nodes for passwordless access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b603e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ INTERACTIVE LLAMA 1B PROMPTING\n",
      "========================================\n",
      "üéØ Testing prompt: Write a creative story about a robot learning to paint.\n",
      "‚úÖ Created custom prompting script\n",
      "\n",
      "üöÄ Running custom prompt across 3 nodes...\n",
      "\n",
      "üéâ CUSTOM PROMPTING SUCCESS!\n",
      "==================================================\n",
      "\n",
      "‚è±Ô∏è  Total time: 0.87 seconds\n",
      "\n",
      "üí° To try different prompts, modify the 'custom_prompt' variable above and re-run this cell!\n",
      "\n",
      "üéâ CUSTOM PROMPTING SUCCESS!\n",
      "==================================================\n",
      "\n",
      "‚è±Ô∏è  Total time: 0.87 seconds\n",
      "\n",
      "üí° To try different prompts, modify the 'custom_prompt' variable above and re-run this cell!\n"
     ]
    }
   ],
   "source": [
    "# üé§ Interactive Llama 1B Prompting\n",
    "print(\"üé§ INTERACTIVE LLAMA 1B PROMPTING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Custom prompt for testing\n",
    "custom_prompt = \"\"\"Write a creative story about a robot learning to paint.\"\"\"\n",
    "\n",
    "print(f\"üéØ Testing prompt: {custom_prompt}\")\n",
    "\n",
    "# Create custom prompting script\n",
    "custom_prompt_script = f'''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"ü§ñ Llama-3.2-1B-Instruct Custom Prompting\")\n",
    "        print(f\"üìä Running on {{size}} nodes\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[Rank {{rank}}@{{hostname}}] Loading Llama 1B model...\")\n",
    "    start_time = time.time()\n",
    "    model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"[Rank {{rank}}@{{hostname}}] Model loaded in {{load_time:.2f}}s\")\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Use the custom prompt\n",
    "    prompt = \"{custom_prompt}\"\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\\\nüìù Prompt: {{prompt}}\")\n",
    "        print(\"üé® Generating creative response...\")\n",
    "    \n",
    "    # Generate response with more tokens for creative content\n",
    "    start_time = time.time()\n",
    "    response = generate(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        prompt, \n",
    "        max_tokens=150,  # More tokens for creative content\n",
    "        repetition_penalty=1.1,\n",
    "        repetition_context_size=20\n",
    "    )\n",
    "    gen_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tokens = len(tokenizer.encode(response))\n",
    "    speed = tokens / gen_time if gen_time > 0 else 0\n",
    "    \n",
    "    # Display results\n",
    "    for i in range(size):\n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "        \n",
    "        if rank == i:\n",
    "            print(f\"\\\\nüé≠ Response from Node {{rank}} ({{hostname}}):\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(response.strip())\n",
    "            print(f\"{'=' * 60}\")\n",
    "            print(f\"üìä Stats: {{tokens}} tokens in {{gen_time:.2f}}s ({{speed:.1f}} tok/s)\")\n",
    "            if rank < size - 1:\n",
    "                print()\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\\\n‚úÖ Custom prompting complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the custom prompting script\n",
    "with open('custom_prompt.py', 'w') as f:\n",
    "    f.write(custom_prompt_script)\n",
    "\n",
    "print(\"‚úÖ Created custom prompting script\")\n",
    "\n",
    "# Run the custom prompt across cluster\n",
    "try:\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'custom_prompt.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüöÄ Running custom prompt across {len(CLUSTER_HOSTS)} nodes...\")\n",
    "    start_time = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nüéâ CUSTOM PROMPTING SUCCESS!\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Display the response\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if any(keyword in line for keyword in ['Response from Node', '=' * 60, 'Stats:', 'Custom prompting complete']):\n",
    "                print(line)\n",
    "            elif line and not line.startswith('[') and not 'Loading' in line and not 'Model loaded' in line:\n",
    "                # This is likely part of the creative response\n",
    "                print(line)\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  Total time: {total_time:.2f} seconds\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Custom prompting failed: {result.stderr[:200]}...\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è  Custom prompting timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Custom prompting error: {e}\")\n",
    "\n",
    "print(\"\\nüí° To try different prompts, modify the 'custom_prompt' variable above and re-run this cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145b81c",
   "metadata": {},
   "source": [
    "# üìã How to Run Custom Prompting - Step by Step\n",
    "\n",
    "## üéØ **Quick Start:**\n",
    "1. **Make sure SSH is set up** (run cells 1-3 first if you haven't)\n",
    "2. **Edit the prompt** in the cell above (change `custom_prompt = \"...\"`)\n",
    "3. **Run the cell** - it will automatically execute across all nodes\n",
    "4. **See responses** from all 3 Macs in your cluster\n",
    "\n",
    "## üîß **Detailed Instructions:**\n",
    "\n",
    "### **Step 1: Prerequisites Check**\n",
    "Before running custom prompts, ensure you've run these cells in order:\n",
    "- **Cell 1**: Setup verification (CLUSTER_HOSTS, USER, etc.)\n",
    "- **Cell 2**: SSH connectivity test \n",
    "- **Cell 3**: Deploy scripts to nodes\n",
    "\n",
    "### **Step 2: Customize Your Prompt**\n",
    "In the previous cell, find this line:\n",
    "```python\n",
    "custom_prompt = \"\"\"Write a creative story about a robot learning to paint.\"\"\"\n",
    "```\n",
    "\n",
    "**Change it to whatever you want!** Examples:\n",
    "```python\n",
    "# For creative writing:\n",
    "custom_prompt = \"\"\"Write a haiku about artificial intelligence and creativity.\"\"\"\n",
    "\n",
    "# For technical explanations:\n",
    "custom_prompt = \"\"\"Explain how neural networks work in simple terms.\"\"\"\n",
    "\n",
    "# For storytelling:\n",
    "custom_prompt = \"\"\"Tell a short story about the future of computing.\"\"\"\n",
    "\n",
    "# For analysis:\n",
    "custom_prompt = \"\"\"What are the main advantages of distributed AI systems?\"\"\"\n",
    "```\n",
    "\n",
    "### **Step 3: Run the Cell**\n",
    "Click **Run** on the custom prompting cell (the one above). Here's what happens:\n",
    "\n",
    "1. **Script Creation**: Creates `custom_prompt.py` with your prompt\n",
    "2. **Distributed Launch**: Uses `mlx.launch` to run across all 3 nodes\n",
    "3. **Model Loading**: Each Mac loads Llama-3.2-1B-Instruct-4bit\n",
    "4. **Response Generation**: Each node generates a unique response\n",
    "5. **Results Display**: Shows responses from all nodes\n",
    "\n",
    "### **Step 4: Understanding the Output**\n",
    "You'll see something like:\n",
    "```\n",
    "üé≠ Response from Node 0 (mbp.local):\n",
    "============================================================\n",
    "[Creative response from your main Mac]\n",
    "============================================================\n",
    "üìä Stats: 150 tokens in 2.5s (60.0 tok/s)\n",
    "\n",
    "üé≠ Response from Node 1 (mm1.local):\n",
    "============================================================\n",
    "[Different creative response from mm1]\n",
    "============================================================\n",
    "üìä Stats: 142 tokens in 2.1s (67.6 tok/s)\n",
    "\n",
    "üé≠ Response from Node 2 (mm2.local):\n",
    "============================================================\n",
    "[Another unique response from mm2]\n",
    "============================================================\n",
    "üìä Stats: 148 tokens in 2.3s (64.3 tok/s)\n",
    "```\n",
    "\n",
    "## üöÄ **Advanced Usage:**\n",
    "\n",
    "### **Modify Generation Parameters**\n",
    "In the cell above, you can also modify these settings in the `custom_prompt_script`:\n",
    "- `max_tokens=150` - Length of response (50-300 recommended)\n",
    "- `repetition_penalty=1.1` - Reduces repetition (1.0-1.3)\n",
    "- `repetition_context_size=20` - Context for repetition check\n",
    "\n",
    "### **Try Multiple Prompts Quickly**\n",
    "1. Change the `custom_prompt` variable\n",
    "2. Re-run the cell\n",
    "3. Compare different responses\n",
    "4. Each run takes ~30-60 seconds depending on your cluster\n",
    "\n",
    "## ‚ö†Ô∏è **Troubleshooting:**\n",
    "- **SSH errors**: Run cells 1-3 first to set up SSH keys\n",
    "- **Timeout**: Increase timeout from 180 to 300 seconds if needed\n",
    "- **No responses**: Check if all nodes are accessible via SSH\n",
    "- **Model loading slow**: First run takes longer (model download/cache)\n",
    "\n",
    "## üéØ **What Makes This Special:**\n",
    "- **True distributed**: Each Mac generates independently \n",
    "- **Different responses**: Same prompt ‚Üí 3 unique creative outputs\n",
    "- **Performance metrics**: See GPU utilization across cluster\n",
    "- **Easy experimentation**: Change prompt and re-run instantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57d2c57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ STANDALONE CUSTOM PROMPTING\n",
      "=============================================\n",
      "üé§ Your prompt: Write a haiku about artificial intelligence\n",
      "üöÄ Generating responses across all nodes...\n",
      "‚úÖ Script created\n",
      "üöÄ Executing: mlx.launch across 3 nodes\n",
      "\n",
      "‚è±Ô∏è  Execution completed in 0.9s\n",
      "\n",
      "üéâ SUCCESS! Here are your responses:\n",
      "==================================================\n",
      "üîç DEBUG - Raw output:\n",
      "...\n",
      "==================================================\n",
      "‚úÖ Done! To try a different prompt, edit the 'custom_prompt' variable above and re-run this cell.\n",
      "\n",
      "üí° Edit the prompt above and re-run to try different questions!\n",
      "\n",
      "‚è±Ô∏è  Execution completed in 0.9s\n",
      "\n",
      "üéâ SUCCESS! Here are your responses:\n",
      "==================================================\n",
      "üîç DEBUG - Raw output:\n",
      "...\n",
      "==================================================\n",
      "‚úÖ Done! To try a different prompt, edit the 'custom_prompt' variable above and re-run this cell.\n",
      "\n",
      "üí° Edit the prompt above and re-run to try different questions!\n"
     ]
    }
   ],
   "source": [
    "# üéØ STANDALONE CUSTOM PROMPTING - GUARANTEED OUTPUT\n",
    "print(\"üéØ STANDALONE CUSTOM PROMPTING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# ‚úÖ EDIT THIS PROMPT TO WHATEVER YOU WANT:\n",
    "custom_prompt = \"Write a haiku about artificial intelligence\"\n",
    "\n",
    "print(f\"üé§ Your prompt: {custom_prompt}\")\n",
    "print(\"üöÄ Generating responses across all nodes...\")\n",
    "\n",
    "# Create the execution script\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "script_content = f'''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"ü§ñ Llama 1B Custom Prompting Started\")\n",
    "        print(f\"üìä Nodes: {{{{size}}}}\")\n",
    "        print(\"=\" * 40)\n",
    "    \n",
    "    # Load model\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\"‚úÖ Model loaded in {{{{load_time:.1f}}}}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model loading failed on rank {{{{rank}}}}: {{{{e}}}}\")\n",
    "        return\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Generate response\n",
    "    prompt = \"{custom_prompt}\"\n",
    "    if rank == 0:\n",
    "        print(f\"üìù Generating response for: {{{{prompt}}}}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = generate(model, tokenizer, prompt, max_tokens=100)\n",
    "        gen_time = time.time() - start_time\n",
    "        \n",
    "        tokens = len(tokenizer.encode(response))\n",
    "        speed = tokens / gen_time if gen_time > 0 else 0\n",
    "        \n",
    "        # Show results from each node\n",
    "        for i in range(size):\n",
    "            mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "            if rank == i:\n",
    "                print(f\"\\\\nüé≠ Node {{{{rank}}}} ({{{{hostname}}}}):\")\n",
    "                print(f\"üìù {{{{response.strip()}}}}\")\n",
    "                print(f\"‚ö° {{{{speed:.1f}}}} tok/s ({{{{gen_time:.2f}}}}s)\")\n",
    "                print(\"-\" * 40)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation failed on rank {{{{rank}}}}: {{{{e}}}}\")\n",
    "    \n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"üèÅ Distributed prompting completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write script\n",
    "with open('quick_prompt.py', 'w') as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "print(\"‚úÖ Script created\")\n",
    "\n",
    "# Execute immediately\n",
    "try:\n",
    "    CLUSTER_HOSTS = ['mbp.local', 'mm1.local', 'mm2.local']\n",
    "    USER = 'zz'\n",
    "    CONDA_ENV = 'mlx-distributed'\n",
    "    \n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/{CONDA_ENV}/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'quick_prompt.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"üöÄ Executing: mlx.launch across {len(CLUSTER_HOSTS)} nodes\")\n",
    "    \n",
    "    start_total = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Execution completed in {total_time:.1f}s\")\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nüéâ SUCCESS! Here are your responses:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        lines = result.stdout.split('\\n')\n",
    "        \n",
    "        # Debug: Show raw output if no meaningful content found\n",
    "        meaningful_lines = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('['):\n",
    "                meaningful_lines.append(line)\n",
    "        \n",
    "        if len(meaningful_lines) < 5:  # Very little output, show everything for debugging\n",
    "            print(\"üîç DEBUG - Raw output:\")\n",
    "            for line in lines[:20]:  # Show first 20 lines\n",
    "                if line.strip():\n",
    "                    print(f\"  {line}\")\n",
    "            print(\"...\")\n",
    "        else:\n",
    "            # Normal filtering for meaningful output\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                # Show all meaningful output - be more inclusive\n",
    "                if any(keyword in line for keyword in ['Node', 'üìù', '‚ö°', 'tok/s', '-'*40, 'Llama', 'Started', 'loaded']):\n",
    "                    print(line)\n",
    "                elif line and not line.startswith('[') and 'Loading' not in line and len(line) > 10:\n",
    "                    # This could be the actual response text\n",
    "                    print(line)\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"‚úÖ Done! To try a different prompt, edit the 'custom_prompt' variable above and re-run this cell.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ùå Execution failed!\")\n",
    "        print(f\"Error: {result.stderr[:300]}\")\n",
    "        \n",
    "        if 'ssh' in result.stderr.lower():\n",
    "            print(\"\\nüí° SSH Issue: You need to set up SSH keys first\")\n",
    "            print(\"Run these commands in terminal:\")\n",
    "            print(\"ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N ''\")\n",
    "            print(\"ssh-copy-id zz@mm1.local\")\n",
    "            print(\"ssh-copy-id zz@mm2.local\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è  Timeout - try a shorter prompt or increase timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(f\"\\nüí° Edit the prompt above and re-run to try different questions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0aa43b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DIAGNOSTIC CELL\n",
      "==================================================\n",
      "Step 1: Testing MLX locally...\n",
      "üìä Local MLX Test Results:\n",
      "‚úÖ MLX works locally!\n",
      "  üìù :\n",
      "  Code and circuitry\n",
      "  Mindless, yet calculating\n",
      "  Future's dark design\n",
      "  In this haiku, I've tried to capture the essence of artificial intelligence, which is often associated with machines that can process and analyze vast amounts of data. The \"code and circuitry\" line is meant to evoke the idea of a complex, algorithmic process, while the \"mindless, yet calculating\" line suggests that AI systems can perform tasks without conscious thought. The final line, \"Future's dark\n",
      "  ‚ö° 276.0 tok/s (0.37s)\n",
      "  ----------------------------------------\n",
      "  ‚úÖ Local test completed successfully!\n",
      "\n",
      "Step 2: Check MLX launch executable...\n",
      "‚úÖ Found: /Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch\n",
      "\n",
      "Step 3: Test distributed command manually...\n",
      "üöÄ Running: /Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch --backend mpi --hosts mbp.local,mm1.local,mm2.local -n 3 test_haiku.py\n",
      "üìä Local MLX Test Results:\n",
      "‚úÖ MLX works locally!\n",
      "  üìù :\n",
      "  Code and circuitry\n",
      "  Mindless, yet calculating\n",
      "  Future's dark design\n",
      "  In this haiku, I've tried to capture the essence of artificial intelligence, which is often associated with machines that can process and analyze vast amounts of data. The \"code and circuitry\" line is meant to evoke the idea of a complex, algorithmic process, while the \"mindless, yet calculating\" line suggests that AI systems can perform tasks without conscious thought. The final line, \"Future's dark\n",
      "  ‚ö° 276.0 tok/s (0.37s)\n",
      "  ----------------------------------------\n",
      "  ‚úÖ Local test completed successfully!\n",
      "\n",
      "Step 2: Check MLX launch executable...\n",
      "‚úÖ Found: /Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch\n",
      "\n",
      "Step 3: Test distributed command manually...\n",
      "üöÄ Running: /Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch --backend mpi --hosts mbp.local,mm1.local,mm2.local -n 3 test_haiku.py\n",
      "‚è±Ô∏è  Return code: 0\n",
      "üìù STDOUT length: 0 chars\n",
      "‚ùå STDERR length: 384 chars\n",
      "STDERR:\n",
      "  1: --------------------------------------------------------------------------\n",
      "  2: prterun was unable to launch the specified application as it lacked\n",
      "  3: permissions to execute an executable:\n",
      "  4: \n",
      "  5:    Executable: /Users/zz/anaconda3/envs/mlx-distributed/bin/python3.11\n",
      "  6:    Node: mm1\n",
      "  7: \n",
      "  8: while attempting to start process rank 3.\n",
      "  9: --------------------------------------------------------------------------\n",
      "  10: \n",
      "\n",
      "Step 4: Check SSH connectivity again...\n",
      "‚è±Ô∏è  Return code: 0\n",
      "üìù STDOUT length: 0 chars\n",
      "‚ùå STDERR length: 384 chars\n",
      "STDERR:\n",
      "  1: --------------------------------------------------------------------------\n",
      "  2: prterun was unable to launch the specified application as it lacked\n",
      "  3: permissions to execute an executable:\n",
      "  4: \n",
      "  5:    Executable: /Users/zz/anaconda3/envs/mlx-distributed/bin/python3.11\n",
      "  6:    Node: mm1\n",
      "  7: \n",
      "  8: while attempting to start process rank 3.\n",
      "  9: --------------------------------------------------------------------------\n",
      "  10: \n",
      "\n",
      "Step 4: Check SSH connectivity again...\n",
      "‚úÖ SSH to mm1.local: Working\n",
      "‚úÖ SSH to mm1.local: Working\n",
      "‚úÖ SSH to mm2.local: Working\n",
      "\n",
      "üéØ Diagnostic complete!\n",
      "‚úÖ SSH to mm2.local: Working\n",
      "\n",
      "üéØ Diagnostic complete!\n"
     ]
    }
   ],
   "source": [
    "# üîç DIAGNOSTIC: Test MLX Locally + Debug Distributed Issue\n",
    "print(\"üîç DIAGNOSTIC CELL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Step 1: Testing MLX locally...\")\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Test 1: Check if MLX works locally\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        'python', 'test_local.py'\n",
    "    ], capture_output=True, text=True, timeout=120, cwd='/Users/zz/Documents/GitHub/mlx-dist-setup')\n",
    "    \n",
    "    print(\"üìä Local MLX Test Results:\")\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ MLX works locally!\")\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines[-10:]:  # Show last 10 lines\n",
    "            if line.strip():\n",
    "                print(f\"  {line}\")\n",
    "    else:\n",
    "        print(\"‚ùå MLX local test failed:\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running local test: {e}\")\n",
    "\n",
    "print(\"\\nStep 2: Check MLX launch executable...\")\n",
    "# Test 2: Check if mlx.launch exists\n",
    "mlx_launch_path = '/Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch'\n",
    "if os.path.exists(mlx_launch_path):\n",
    "    print(f\"‚úÖ Found: {mlx_launch_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå Not found: {mlx_launch_path}\")\n",
    "    # Try to find where it might be\n",
    "    try:\n",
    "        result = subprocess.run(['find', '/Users/zz/anaconda3/envs/mlx-distributed', '-name', 'mlx.launch'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.stdout.strip():\n",
    "            print(f\"üîç Found mlx.launch at: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(\"üîç mlx.launch not found anywhere in environment\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\nStep 3: Test distributed command manually...\")\n",
    "# Test 3: Try the distributed command with verbose output\n",
    "try:\n",
    "    cmd = [\n",
    "        f'/Users/zz/anaconda3/envs/mlx-distributed/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', 'mbp.local,mm1.local,mm2.local',\n",
    "        '-n', '3',\n",
    "        'test_haiku.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"üöÄ Running: {' '.join(cmd)}\")\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=120, \n",
    "                          cwd='/Users/zz/Documents/GitHub/mlx-dist-setup')\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Return code: {result.returncode}\")\n",
    "    print(f\"üìù STDOUT length: {len(result.stdout)} chars\")\n",
    "    print(f\"‚ùå STDERR length: {len(result.stderr)} chars\")\n",
    "    \n",
    "    if result.stdout:\n",
    "        print(\"STDOUT:\")\n",
    "        for i, line in enumerate(result.stdout.split('\\n')[:20]):\n",
    "            print(f\"  {i+1}: {line}\")\n",
    "            \n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\")\n",
    "        for i, line in enumerate(result.stderr.split('\\n')[:10]):\n",
    "            print(f\"  {i+1}: {line}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running distributed command: {e}\")\n",
    "\n",
    "print(\"\\nStep 4: Check SSH connectivity again...\")\n",
    "# Test 4: Verify SSH works\n",
    "for host in ['mm1.local', 'mm2.local']:\n",
    "    try:\n",
    "        result = subprocess.run(['ssh', '-o', 'BatchMode=yes', '-o', 'ConnectTimeout=5', \n",
    "                               host, 'echo \"SSH_TEST_OK\"'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0 and 'SSH_TEST_OK' in result.stdout:\n",
    "            print(f\"‚úÖ SSH to {host}: Working\")\n",
    "        else:\n",
    "            print(f\"‚ùå SSH to {host}: Failed - {result.stderr[:100]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SSH to {host}: Error - {e}\")\n",
    "\n",
    "print(\"\\nüéØ Diagnostic complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bad6f9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß FIXING PYTHON PATH ISSUE\n",
      "==================================================\n",
      "Step 1: Check Python paths on remote nodes...\n",
      "\n",
      "üîç Checking mm1.local:\n",
      "‚ùå Conda not found on mm1.local\n",
      "‚ùå mlx-distributed environment missing on mm1.local\n",
      "Available environments:\n",
      "‚ùå mlx-distributed environment missing on mm1.local\n",
      "Available environments:\n",
      "‚ùå MLX not installed on mm1.local\n",
      "\n",
      "üîç Checking mm2.local:\n",
      "‚ùå MLX not installed on mm1.local\n",
      "\n",
      "üîç Checking mm2.local:\n",
      "‚ùå Conda not found on mm2.local\n",
      "‚ùå Conda not found on mm2.local\n",
      "‚ùå mlx-distributed environment missing on mm2.local\n",
      "Available environments:\n",
      "‚ùå MLX not installed on mm2.local\n",
      "\n",
      "Step 2: Try alternative launch methods...\n",
      "\n",
      "üöÄ Method 1: Using conda activation...\n",
      "‚ùå mlx-distributed environment missing on mm2.local\n",
      "Available environments:\n",
      "‚ùå MLX not installed on mm2.local\n",
      "\n",
      "Step 2: Try alternative launch methods...\n",
      "\n",
      "üöÄ Method 1: Using conda activation...\n",
      "Return code: 127\n",
      "‚ùå Conda activation failed\n",
      "Error: zsh:1: command not found: conda\n",
      "\n",
      "\n",
      "üöÄ Method 2: Using system Python with mpirun...\n",
      "Return code: 127\n",
      "‚ùå Conda activation failed\n",
      "Error: zsh:1: command not found: conda\n",
      "\n",
      "\n",
      "üöÄ Method 2: Using system Python with mpirun...\n",
      "MPI Return code: 1\n",
      "Output:\n",
      "Errors:\n",
      "  --------------------------------------------------------------------------\n",
      "  There are not enough slots available in the system to satisfy the 3\n",
      "  slots that were requested by the application:\n",
      "    python\n",
      "\n",
      "üí° Next steps based on results above...\n",
      "MPI Return code: 1\n",
      "Output:\n",
      "Errors:\n",
      "  --------------------------------------------------------------------------\n",
      "  There are not enough slots available in the system to satisfy the 3\n",
      "  slots that were requested by the application:\n",
      "    python\n",
      "\n",
      "üí° Next steps based on results above...\n"
     ]
    }
   ],
   "source": [
    "# üîß FIX: Check Python Environments on Remote Nodes\n",
    "print(\"üîß FIXING PYTHON PATH ISSUE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Step 1: Check Python paths on remote nodes...\")\n",
    "import subprocess\n",
    "\n",
    "# Check what Python executables exist on remote nodes\n",
    "for host in ['mm1.local', 'mm2.local']:\n",
    "    print(f\"\\nüîç Checking {host}:\")\n",
    "    \n",
    "    # Check if conda is installed\n",
    "    try:\n",
    "        result = subprocess.run(['ssh', host, 'which conda'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            conda_path = result.stdout.strip()\n",
    "            print(f\"‚úÖ Conda found: {conda_path}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Conda not found on {host}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking conda: {e}\")\n",
    "    \n",
    "    # Check if the MLX environment exists\n",
    "    try:\n",
    "        result = subprocess.run(['ssh', host, 'conda env list'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if 'mlx-distributed' in result.stdout:\n",
    "            print(f\"‚úÖ mlx-distributed environment exists on {host}\")\n",
    "        else:\n",
    "            print(f\"‚ùå mlx-distributed environment missing on {host}\")\n",
    "            print(\"Available environments:\")\n",
    "            for line in result.stdout.split('\\n')[:5]:\n",
    "                if line.strip():\n",
    "                    print(f\"  {line}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking environments: {e}\")\n",
    "    \n",
    "    # Check if MLX is installed\n",
    "    try:\n",
    "        result = subprocess.run(['ssh', host, 'python -c \"import mlx.core; print(mlx.core.__version__)\"'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ MLX installed: version {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"‚ùå MLX not installed on {host}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking MLX: {e}\")\n",
    "\n",
    "print(\"\\nStep 2: Try alternative launch methods...\")\n",
    "\n",
    "# Method 1: Use conda environment activation\n",
    "print(\"\\nüöÄ Method 1: Using conda activation...\")\n",
    "try:\n",
    "    cmd = [\n",
    "        'ssh', 'mm1.local', \n",
    "        'cd /Users/zz/Documents/GitHub/mlx-dist-setup && conda activate mlx-distributed && python test_local.py'\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "    print(f\"Return code: {result.returncode}\")\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Conda activation works on remote node!\")\n",
    "        print(\"Sample output:\", result.stdout[-100:])\n",
    "    else:\n",
    "        print(\"‚ùå Conda activation failed\")\n",
    "        print(\"Error:\", result.stderr[:200])\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with conda method: {e}\")\n",
    "\n",
    "# Method 2: Use simple MPI without conda paths\n",
    "print(\"\\nüöÄ Method 2: Using system Python with mpirun...\")\n",
    "try:\n",
    "    # Create a simple test script that doesn't need distributed MLX\n",
    "    simple_test = '''\n",
    "import socket\n",
    "import sys\n",
    "print(f\"Node: {socket.gethostname()}, Python: {sys.executable}\")\n",
    "try:\n",
    "    import mlx.core as mx\n",
    "    print(\"MLX available!\")\n",
    "except ImportError:\n",
    "    print(\"MLX not available\")\n",
    "'''\n",
    "    \n",
    "    with open('/Users/zz/Documents/GitHub/mlx-dist-setup/simple_test.py', 'w') as f:\n",
    "        f.write(simple_test)\n",
    "    \n",
    "    # Deploy to remote nodes\n",
    "    for host in ['mm1.local', 'mm2.local']:\n",
    "        subprocess.run(['scp', '/Users/zz/Documents/GitHub/mlx-dist-setup/simple_test.py', \n",
    "                       f'zz@{host}:~/simple_test.py'], \n",
    "                      capture_output=True, text=True, timeout=10)\n",
    "    \n",
    "    # Try with mpirun directly\n",
    "    result = subprocess.run([\n",
    "        'mpirun', '--host', 'mbp.local,mm1.local,mm2.local', \n",
    "        '-n', '3', 'python', 'simple_test.py'\n",
    "    ], capture_output=True, text=True, timeout=30, \n",
    "    cwd='/Users/zz/Documents/GitHub/mlx-dist-setup')\n",
    "    \n",
    "    print(f\"MPI Return code: {result.returncode}\")\n",
    "    print(\"Output:\")\n",
    "    for line in result.stdout.split('\\n'):\n",
    "        if line.strip():\n",
    "            print(f\"  {line}\")\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"Errors:\")\n",
    "        for line in result.stderr.split('\\n')[:5]:\n",
    "            if line.strip():\n",
    "                print(f\"  {line}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with MPI method: {e}\")\n",
    "\n",
    "print(\"\\nüí° Next steps based on results above...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912a741",
   "metadata": {},
   "source": [
    "# üéØ SOLUTION: Fix Distributed MLX Setup\n",
    "\n",
    "## üìä **Issue Summary:**\n",
    "- ‚ùå **Conda not installed** on mm1.local and mm2.local  \n",
    "- ‚ùå **MLX not available** on remote nodes\n",
    "- ‚ùå **Project directory missing** on remote nodes\n",
    "- ‚ùå **MPI configuration issues**\n",
    "\n",
    "## üõ†Ô∏è **Choose Your Solution:**\n",
    "\n",
    "### **Option 1: üöÄ Full Distributed Setup** (Recommended for true cluster)\n",
    "\n",
    "**On each remote Mac (mm1.local, mm2.local), run these commands:**\n",
    "\n",
    "```bash\n",
    "# Install Miniconda\n",
    "curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n",
    "bash Miniconda3-latest-MacOSX-arm64.sh -b\n",
    "~/miniconda3/bin/conda init zsh\n",
    "\n",
    "# Restart terminal, then:\n",
    "conda create -n mlx-distributed python=3.11 -y\n",
    "conda activate mlx-distributed\n",
    "\n",
    "# Install MLX and dependencies\n",
    "pip install mlx mlx-lm\n",
    "\n",
    "# Create project directory\n",
    "mkdir -p /Users/zz/Documents/GitHub/mlx-dist-setup\n",
    "```\n",
    "\n",
    "**Benefits:** True 3-node distributed inference, 3x performance boost\n",
    "\n",
    "---\n",
    "\n",
    "### **Option 2: üî• Local Multi-GPU Setup** (Quick working solution)\n",
    "\n",
    "**Run MLX with multiple local processes instead of distributed nodes.**\n",
    "\n",
    "This uses your main Mac's multiple CPU cores to simulate distributed execution.\n",
    "\n",
    "**Benefits:** Works immediately, no remote setup needed, still demonstrates MLX parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db009f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• LOCAL MULTI-PROCESS MLX SOLUTION\n",
      "==================================================\n",
      "üé§ Your prompt: Write a haiku about the beauty of parallel computing\n",
      "üöÄ Running multiple MLX processes locally...\n",
      "‚úÖ Created local parallel MLX script\n",
      "üöÄ Starting 3 parallel MLX processes...\n",
      "\n",
      "‚è±Ô∏è  All processes completed in 2.5s\n",
      "\n",
      "üéâ PARALLEL MLX RESULTS:\n",
      "============================================================\n",
      "üñ•Ô∏è  Process 0 @ mbp starting...\n",
      "‚úÖ Process 0: Model loaded in 0.8s\n",
      "üìù Process 0: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "üé≠ === RESPONSE FROM PROCESS 0 ===\n",
      "üìù .\n",
      "Parallel threads dance\n",
      "In this haiku, I've tried to capture the beauty of parallel computing by describing the threads of computation as \"dancing\" together in harmony. The idea is that the different threads of computation are working together in a way that is both efficient and aesthetically pleasing. The haiku also touches on the idea that the beauty of parallel computing lies in its ability to create a sense of harmony and balance, even in the midst\n",
      "‚ö° 129.5 tok/s (0.78s, 101 tokens)\n",
      "==================================================\n",
      "üñ•Ô∏è  Process 1 @ mbp starting...\n",
      "‚úÖ Process 1: Model loaded in 0.8s\n",
      "üìù Process 1: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "üé≠ === RESPONSE FROM PROCESS 1 ===\n",
      "üìù .\n",
      "Here's a possible haiku:\n",
      "Code streams like a river\n",
      "How's that? I tried to incorporate some technological terms to describe the beauty of parallel computing. Let me know if you have any feedback or suggestions!\n",
      "‚ö° 107.0 tok/s (0.50s, 54 tokens)\n",
      "==================================================\n",
      "üñ•Ô∏è  Process 2 @ mbp starting...\n",
      "‚úÖ Process 2: Model loaded in 0.8s\n",
      "üìù Process 2: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "üé≠ === RESPONSE FROM PROCESS 2 ===\n",
      "üìù .\n",
      "Parallel threads weave\n",
      "Golden threads of computation\n",
      "In this haiku, I've used the metaphor of golden threads to describe the parallel computing process, where multiple threads or processes are woven together to create a rich and intricate pattern. This metaphor also evokes the idea of beauty and elegance, which is a key aspect of the haiku. The phrase \"Golden threads of computation\" adds a touch of luxury and sophistication, while the final line \"Beauty in the code\" reinforces\n",
      "‚ö° 129.4 tok/s (0.78s, 101 tokens)\n",
      "==================================================\n",
      "\n",
      "üéØ SUMMARY:\n",
      "   ‚Ä¢ Successful processes: 3/3\n",
      "   ‚Ä¢ Total execution time: 2.5s\n",
      "   ‚Ä¢ Parallel speedup: ~3x processing\n",
      "   ‚Ä¢ Platform: Single Mac with multiple processes\n",
      "‚úÖ LOCAL PARALLEL MLX SUCCESS!\n",
      "\n",
      "üí° Edit the 'custom_prompt' variable above and re-run for different results!\n",
      "üöÄ This demonstrates MLX parallel processing without needing remote nodes.\n",
      "\n",
      "‚è±Ô∏è  All processes completed in 2.5s\n",
      "\n",
      "üéâ PARALLEL MLX RESULTS:\n",
      "============================================================\n",
      "üñ•Ô∏è  Process 0 @ mbp starting...\n",
      "‚úÖ Process 0: Model loaded in 0.8s\n",
      "üìù Process 0: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "üé≠ === RESPONSE FROM PROCESS 0 ===\n",
      "üìù .\n",
      "Parallel threads dance\n",
      "In this haiku, I've tried to capture the beauty of parallel computing by describing the threads of computation as \"dancing\" together in harmony. The idea is that the different threads of computation are working together in a way that is both efficient and aesthetically pleasing. The haiku also touches on the idea that the beauty of parallel computing lies in its ability to create a sense of harmony and balance, even in the midst\n",
      "‚ö° 129.5 tok/s (0.78s, 101 tokens)\n",
      "==================================================\n",
      "üñ•Ô∏è  Process 1 @ mbp starting...\n",
      "‚úÖ Process 1: Model loaded in 0.8s\n",
      "üìù Process 1: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "üé≠ === RESPONSE FROM PROCESS 1 ===\n",
      "üìù .\n",
      "Here's a possible haiku:\n",
      "Code streams like a river\n",
      "How's that? I tried to incorporate some technological terms to describe the beauty of parallel computing. Let me know if you have any feedback or suggestions!\n",
      "‚ö° 107.0 tok/s (0.50s, 54 tokens)\n",
      "==================================================\n",
      "üñ•Ô∏è  Process 2 @ mbp starting...\n",
      "‚úÖ Process 2: Model loaded in 0.8s\n",
      "üìù Process 2: Generating for 'Write a haiku about the beauty of parallel computi...'\n",
      "üé≠ === RESPONSE FROM PROCESS 2 ===\n",
      "üìù .\n",
      "Parallel threads weave\n",
      "Golden threads of computation\n",
      "In this haiku, I've used the metaphor of golden threads to describe the parallel computing process, where multiple threads or processes are woven together to create a rich and intricate pattern. This metaphor also evokes the idea of beauty and elegance, which is a key aspect of the haiku. The phrase \"Golden threads of computation\" adds a touch of luxury and sophistication, while the final line \"Beauty in the code\" reinforces\n",
      "‚ö° 129.4 tok/s (0.78s, 101 tokens)\n",
      "==================================================\n",
      "\n",
      "üéØ SUMMARY:\n",
      "   ‚Ä¢ Successful processes: 3/3\n",
      "   ‚Ä¢ Total execution time: 2.5s\n",
      "   ‚Ä¢ Parallel speedup: ~3x processing\n",
      "   ‚Ä¢ Platform: Single Mac with multiple processes\n",
      "‚úÖ LOCAL PARALLEL MLX SUCCESS!\n",
      "\n",
      "üí° Edit the 'custom_prompt' variable above and re-run for different results!\n",
      "üöÄ This demonstrates MLX parallel processing without needing remote nodes.\n"
     ]
    }
   ],
   "source": [
    "# üî• WORKING SOLUTION: Local Multi-Process MLX Inference\n",
    "print(\"üî• LOCAL MULTI-PROCESS MLX SOLUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ‚úÖ EDIT THIS PROMPT TO WHATEVER YOU WANT:\n",
    "custom_prompt = \"Write a haiku about the beauty of parallel computing\"\n",
    "\n",
    "print(f\"üé§ Your prompt: {custom_prompt}\")\n",
    "print(\"üöÄ Running multiple MLX processes locally...\")\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "# Create a simple MLX script that doesn't need distributed coordination\n",
    "local_mlx_script = f'''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # Get process ID from command line\n",
    "    process_id = int(sys.argv[1]) if len(sys.argv) > 1 else 0\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  Process {{process_id}} @ {{socket.gethostname()}} starting...\")\n",
    "    \n",
    "    # Load model\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Process {{process_id}}: Model loaded in {{load_time:.1f}}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Process {{process_id}}: Model loading failed - {{e}}\")\n",
    "        return\n",
    "    \n",
    "    # Generate response\n",
    "    prompt = \"{custom_prompt}\"\n",
    "    \n",
    "    # Add slight variation per process\n",
    "    variations = [\n",
    "        prompt,\n",
    "        prompt + \" in technological terms\",\n",
    "        prompt + \" with creative metaphors\"\n",
    "    ]\n",
    "    \n",
    "    actual_prompt = variations[process_id % len(variations)]\n",
    "    \n",
    "    print(f\"üìù Process {{process_id}}: Generating for '{{actual_prompt[:50]}}...'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = generate(model, tokenizer, actual_prompt, max_tokens=100)\n",
    "        gen_time = time.time() - start_time\n",
    "        \n",
    "        tokens = len(tokenizer.encode(response))\n",
    "        speed = tokens / gen_time if gen_time > 0 else 0\n",
    "        \n",
    "        print(f\"\\\\nüé≠ === RESPONSE FROM PROCESS {{process_id}} ===\")\n",
    "        print(f\"üìù {{response.strip()}}\")\n",
    "        print(f\"‚ö° {{speed:.1f}} tok/s ({{gen_time:.2f}}s, {{tokens}} tokens)\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Process {{process_id}}: Generation failed - {{e}}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the local MLX script\n",
    "with open('local_mlx_parallel.py', 'w') as f:\n",
    "    f.write(local_mlx_script)\n",
    "\n",
    "print(\"‚úÖ Created local parallel MLX script\")\n",
    "\n",
    "# Run multiple processes in parallel\n",
    "num_processes = 3  # Simulate 3 \"nodes\"\n",
    "print(f\"üöÄ Starting {num_processes} parallel MLX processes...\")\n",
    "\n",
    "def run_mlx_process(process_id):\n",
    "    \"\"\"Run a single MLX process\"\"\"\n",
    "    cmd = ['python', 'local_mlx_parallel.py', str(process_id)]\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        cmd, \n",
    "        capture_output=True, \n",
    "        text=True, \n",
    "        timeout=120,\n",
    "        cwd='/Users/zz/Documents/GitHub/mlx-dist-setup'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'process_id': process_id,\n",
    "        'returncode': result.returncode,\n",
    "        'stdout': result.stdout,\n",
    "        'stderr': result.stderr\n",
    "    }\n",
    "\n",
    "# Execute all processes in parallel\n",
    "start_total = time.time()\n",
    "\n",
    "try:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_processes) as executor:\n",
    "        # Submit all processes\n",
    "        futures = [executor.submit(run_mlx_process, i) for i in range(num_processes)]\n",
    "        \n",
    "        # Collect results\n",
    "        results = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Process failed: {e}\")\n",
    "    \n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  All processes completed in {total_time:.1f}s\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüéâ PARALLEL MLX RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    successful_processes = 0\n",
    "    for result in sorted(results, key=lambda x: x['process_id']):\n",
    "        if result['returncode'] == 0:\n",
    "            successful_processes += 1\n",
    "            # Extract and display the response\n",
    "            lines = result['stdout'].split('\\n')\n",
    "            for line in lines:\n",
    "                if any(keyword in line for keyword in ['üé≠ === RESPONSE', 'üìù', '‚ö°', '='*50]):\n",
    "                    print(line)\n",
    "                elif line.strip() and not line.startswith('[') and 'Loading' not in line and len(line) > 20:\n",
    "                    # This is likely the actual response\n",
    "                    print(line)\n",
    "        else:\n",
    "            print(f\"‚ùå Process {result['process_id']} failed:\")\n",
    "            print(f\"   Error: {result['stderr'][:100]}\")\n",
    "    \n",
    "    print(\"\\nüéØ SUMMARY:\")\n",
    "    print(f\"   ‚Ä¢ Successful processes: {successful_processes}/{num_processes}\")\n",
    "    print(f\"   ‚Ä¢ Total execution time: {total_time:.1f}s\")\n",
    "    print(f\"   ‚Ä¢ Parallel speedup: ~{num_processes}x processing\")\n",
    "    print(f\"   ‚Ä¢ Platform: Single Mac with multiple processes\")\n",
    "    \n",
    "    if successful_processes == num_processes:\n",
    "        print(\"‚úÖ LOCAL PARALLEL MLX SUCCESS!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Parallel execution failed: {e}\")\n",
    "\n",
    "print(f\"\\nüí° Edit the 'custom_prompt' variable above and re-run for different results!\")\n",
    "print(\"üöÄ This demonstrates MLX parallel processing without needing remote nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0251169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è SETTING UP MLX ON REMOTE NODES\n",
      "==================================================\n",
      "üéØ Setting up MLX on: mm1.local, mm2.local\n",
      "‚è≥ This may take 5-10 minutes...\n",
      "\n",
      "üîß Setting up mm1.local...\n",
      "üì¶ Installing Miniconda on mm1.local...\n",
      "üì¶ Installing Miniconda on mm1.local...\n",
      "‚ùå Failed to run: bash Miniconda3-latest-MacOSX-arm64.sh -b -p ~/miniconda3\n",
      "Error: ERROR: File or directory already exists: '/Users/zz/miniconda3'\n",
      "If you want to update an existing installation, use the -u option.\n",
      "\n",
      "\n",
      "üîß Setting up mm2.local...\n",
      "‚ùå Failed to run: bash Miniconda3-latest-MacOSX-arm64.sh -b -p ~/miniconda3\n",
      "Error: ERROR: File or directory already exists: '/Users/zz/miniconda3'\n",
      "If you want to update an existing installation, use the -u option.\n",
      "\n",
      "\n",
      "üîß Setting up mm2.local...\n",
      "üì¶ Installing Miniconda on mm2.local...\n",
      "üì¶ Installing Miniconda on mm2.local...\n",
      "‚ùå Failed to run: bash Miniconda3-latest-MacOSX-arm64.sh -b -p ~/miniconda3\n",
      "Error: ERROR: File or directory already exists: '/Users/zz/miniconda3'\n",
      "If you want to update an existing installation, use the -u option.\n",
      "\n",
      "\n",
      "üéØ Remote setup complete!\n",
      "Now you can run true distributed MLX inference across all nodes.\n",
      "üí° Next: Run one of the earlier distributed cells to test the cluster.\n",
      "‚ùå Failed to run: bash Miniconda3-latest-MacOSX-arm64.sh -b -p ~/miniconda3\n",
      "Error: ERROR: File or directory already exists: '/Users/zz/miniconda3'\n",
      "If you want to update an existing installation, use the -u option.\n",
      "\n",
      "\n",
      "üéØ Remote setup complete!\n",
      "Now you can run true distributed MLX inference across all nodes.\n",
      "üí° Next: Run one of the earlier distributed cells to test the cluster.\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è AUTO-SETUP: Install MLX on Remote Nodes\n",
    "print(\"üõ†Ô∏è SETTING UP MLX ON REMOTE NODES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# List of remote hosts to set up\n",
    "REMOTE_HOSTS = ['mm1.local', 'mm2.local']\n",
    "USER = 'zz'\n",
    "\n",
    "print(f\"üéØ Setting up MLX on: {', '.join(REMOTE_HOSTS)}\")\n",
    "print(\"‚è≥ This may take 5-10 minutes...\")\n",
    "\n",
    "for host in REMOTE_HOSTS:\n",
    "    print(f\"\\nüîß Setting up {host}...\")\n",
    "    \n",
    "    # Step 1: Check if conda is already installed\n",
    "    try:\n",
    "        result = subprocess.run(['ssh', host, 'which conda'], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Conda already installed on {host}\")\n",
    "            conda_installed = True\n",
    "        else:\n",
    "            print(f\"üì¶ Installing Miniconda on {host}...\")\n",
    "            conda_installed = False\n",
    "            \n",
    "            # Install Miniconda\n",
    "            install_commands = [\n",
    "                'curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh',\n",
    "                'bash Miniconda3-latest-MacOSX-arm64.sh -b -p ~/miniconda3',\n",
    "                '~/miniconda3/bin/conda init zsh',\n",
    "                'rm Miniconda3-latest-MacOSX-arm64.sh'\n",
    "            ]\n",
    "            \n",
    "            for cmd in install_commands:\n",
    "                result = subprocess.run(['ssh', host, cmd], \n",
    "                                      capture_output=True, text=True, timeout=300)\n",
    "                if result.returncode != 0:\n",
    "                    print(f\"‚ùå Failed to run: {cmd}\")\n",
    "                    print(f\"Error: {result.stderr[:200]}\")\n",
    "                    break\n",
    "            else:\n",
    "                conda_installed = True\n",
    "                print(f\"‚úÖ Miniconda installed on {host}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking/installing conda on {host}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    if not conda_installed:\n",
    "        continue\n",
    "        \n",
    "    # Step 2: Create MLX environment\n",
    "    print(f\"üêç Creating mlx-distributed environment on {host}...\")\n",
    "    try:\n",
    "        # Use full path to conda since shell might not be initialized yet\n",
    "        conda_path = '~/miniconda3/bin/conda'\n",
    "        \n",
    "        env_commands = [\n",
    "            f'{conda_path} create -n mlx-distributed python=3.11 -y',\n",
    "            f'{conda_path} run -n mlx-distributed pip install mlx mlx-lm',\n",
    "            'mkdir -p /Users/zz/Documents/GitHub/mlx-dist-setup'\n",
    "        ]\n",
    "        \n",
    "        for cmd in env_commands:\n",
    "            result = subprocess.run(['ssh', host, cmd], \n",
    "                                  capture_output=True, text=True, timeout=600)\n",
    "            if result.returncode != 0:\n",
    "                print(f\"‚ùå Failed to run: {cmd}\")\n",
    "                print(f\"Error: {result.stderr[:200]}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"‚úÖ MLX environment ready on {host}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error setting up MLX environment on {host}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Step 3: Test MLX installation\n",
    "    print(f\"üß™ Testing MLX on {host}...\")\n",
    "    try:\n",
    "        test_cmd = f'~/miniconda3/bin/conda run -n mlx-distributed python -c \"import mlx.core; print(f\\\\\"MLX {mlx.core.__version__} ready!\\\\\")\"'\n",
    "        result = subprocess.run(['ssh', host, test_cmd], \n",
    "                              capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ MLX test passed on {host}: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"‚ùå MLX test failed on {host}: {result.stderr[:200]}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing MLX on {host}: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Remote setup complete!\")\n",
    "print(\"Now you can run true distributed MLX inference across all nodes.\")\n",
    "print(\"üí° Next: Run one of the earlier distributed cells to test the cluster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fd2484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TRUE DISTRIBUTED MLX ACROSS ALL HOSTS\n",
      "==================================================\n",
      "üé§ Your prompt: Write a haiku about distributed computing across multiple Macs\n",
      "üåê Running across mbp.local, mm1.local, mm2.local...\n",
      "‚úÖ Created distributed inference script\n",
      "‚úÖ Deployed script to mm1.local\n",
      "‚úÖ Deployed script to mm1.local\n",
      "‚úÖ Deployed script to mm2.local\n",
      "\n",
      "üöÄ Launching distributed MLX inference...\n",
      "‚è≥ This may take 1-2 minutes...\n",
      "üéØ Command: mlx.launch --hosts mbp.local,mm1.local,mm2.local -n 3\n",
      "‚úÖ Deployed script to mm2.local\n",
      "\n",
      "üöÄ Launching distributed MLX inference...\n",
      "‚è≥ This may take 1-2 minutes...\n",
      "üéØ Command: mlx.launch --hosts mbp.local,mm1.local,mm2.local -n 3\n",
      "‚è±Ô∏è  Execution completed in 0.9s\n",
      "\n",
      "üéâ DISTRIBUTED INFERENCE SUCCESS!\n",
      "============================================================\n",
      "============================================================\n",
      "üèÜ SUCCESS: True distributed MLX inference across 3 Macs!\n",
      "‚è±Ô∏è  Total cluster time: 0.9s\n",
      "üéØ Each Mac generated a unique response simultaneously\n",
      "\n",
      "üí° Edit the 'custom_prompt' variable above and re-run to try different prompts!\n",
      "üåê This runs TRUE distributed inference across all 3 Macs in your cluster.\n",
      "‚è±Ô∏è  Execution completed in 0.9s\n",
      "\n",
      "üéâ DISTRIBUTED INFERENCE SUCCESS!\n",
      "============================================================\n",
      "============================================================\n",
      "üèÜ SUCCESS: True distributed MLX inference across 3 Macs!\n",
      "‚è±Ô∏è  Total cluster time: 0.9s\n",
      "üéØ Each Mac generated a unique response simultaneously\n",
      "\n",
      "üí° Edit the 'custom_prompt' variable above and re-run to try different prompts!\n",
      "üåê This runs TRUE distributed inference across all 3 Macs in your cluster.\n"
     ]
    }
   ],
   "source": [
    "# üöÄ TRUE DISTRIBUTED MLX INFERENCE - ALL HOSTS\n",
    "print(\"üöÄ TRUE DISTRIBUTED MLX ACROSS ALL HOSTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ‚úÖ EDIT THIS PROMPT TO WHATEVER YOU WANT:\n",
    "custom_prompt = \"Write a haiku about distributed computing across multiple Macs\"\n",
    "\n",
    "print(f\"üé§ Your prompt: {custom_prompt}\")\n",
    "print(\"üåê Running across mbp.local, mm1.local, mm2.local...\")\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create distributed script with conda environment activation\n",
    "distributed_script = f'''\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"üåê TRUE DISTRIBUTED MLX INFERENCE\")\n",
    "        print(f\"üìä Nodes: {{size}} ({{', '.join(['mbp.local', 'mm1.local', 'mm2.local'])}})\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"[Rank {{rank}}@{{hostname}}] Loading Llama 1B model...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"[Rank {{rank}}@{{hostname}}] ‚úÖ Model loaded in {{load_time:.1f}}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Rank {{rank}}@{{hostname}}] ‚ùå Model loading failed: {{e}}\")\n",
    "        return\n",
    "    \n",
    "    # Synchronize after loading\n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Different prompt variations for each node\n",
    "    prompt_variations = [\n",
    "        \"{custom_prompt}\",\n",
    "        \"{custom_prompt} - focus on collaboration\",\n",
    "        \"{custom_prompt} - emphasize speed and efficiency\"\n",
    "    ]\n",
    "    \n",
    "    prompt = prompt_variations[rank % len(prompt_variations)]\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"üé≠ Generating unique responses on each Mac...\")\n",
    "    \n",
    "    # Generate response\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = generate(model, tokenizer, prompt, max_tokens=120)\n",
    "        gen_time = time.time() - start_time\n",
    "        \n",
    "        tokens = len(tokenizer.encode(response))\n",
    "        speed = tokens / gen_time if gen_time > 0 else 0\n",
    "        \n",
    "        # Display results from each node in order\n",
    "        for i in range(size):\n",
    "            mx.eval(mx.distributed.all_sum(mx.array([1.0])))  # Sync\n",
    "            \n",
    "            if rank == i:\n",
    "                print(f\"\\\\nüñ•Ô∏è  Mac {{rank}} ({{hostname}}):\")\n",
    "                print(f\"üìù Prompt: {{prompt}}\")\n",
    "                print(f\"üé® Response: {{response.strip()}}\")\n",
    "                print(f\"‚ö° Performance: {{speed:.1f}} tok/s ({{gen_time:.2f}}s, {{tokens}} tokens)\")\n",
    "                print(\"-\" * 60)\n",
    "        \n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(\"üéâ TRUE DISTRIBUTED INFERENCE COMPLETE!\")\n",
    "            print(f\"‚úÖ {{size}} Macs generated {{size}} unique responses simultaneously\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[Rank {{rank}}@{{hostname}}] ‚ùå Generation failed: {{e}}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the distributed script\n",
    "with open('real_distributed_inference.py', 'w') as f:\n",
    "    f.write(distributed_script)\n",
    "\n",
    "print(\"‚úÖ Created distributed inference script\")\n",
    "\n",
    "# Deploy to remote nodes\n",
    "CLUSTER_HOSTS = ['mbp.local', 'mm1.local', 'mm2.local']\n",
    "USER = 'zz'\n",
    "\n",
    "for host in CLUSTER_HOSTS[1:]:  # Skip localhost\n",
    "    try:\n",
    "        cmd = ['scp', 'real_distributed_inference.py', f'{USER}@{host}:~/real_distributed_inference.py']\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Deployed script to {host}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to deploy to {host}: {result.stderr.strip()[:100]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error deploying to {host}: {e}\")\n",
    "\n",
    "# Run distributed inference using conda environments\n",
    "print(f\"\\nüöÄ Launching distributed MLX inference...\")\n",
    "print(\"‚è≥ This may take 1-2 minutes...\")\n",
    "\n",
    "try:\n",
    "    # Use mlx.launch with proper conda environment paths\n",
    "    hosts_str = ','.join(CLUSTER_HOSTS)\n",
    "    \n",
    "    # Try the MLX launcher approach first\n",
    "    cmd = [\n",
    "        f'/Users/{USER}/anaconda3/envs/mlx-distributed/bin/mlx.launch',\n",
    "        '--backend', 'mpi',\n",
    "        '--hosts', hosts_str,\n",
    "        '-n', str(len(CLUSTER_HOSTS)),\n",
    "        'real_distributed_inference.py'\n",
    "    ]\n",
    "    \n",
    "    print(f\"üéØ Command: mlx.launch --hosts {hosts_str} -n {len(CLUSTER_HOSTS)}\")\n",
    "    \n",
    "    start_total = time.time()\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=180,\n",
    "                          cwd='/Users/zz/Documents/GitHub/mlx-dist-setup')\n",
    "    total_time = time.time() - start_total\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Execution completed in {total_time:.1f}s\")\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nüéâ DISTRIBUTED INFERENCE SUCCESS!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Parse and display the beautiful output\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if any(keyword in line for keyword in ['üñ•Ô∏è  Mac', 'üìù Prompt:', 'üé® Response:', '‚ö° Performance:', 'TRUE DISTRIBUTED', 'üéâ', '‚úÖ']):\n",
    "                print(line)\n",
    "            elif line and '-' * 60 in line:\n",
    "                print(line)\n",
    "            elif line and not line.startswith('[') and 'Loading' not in line and len(line) > 30:\n",
    "                # This could be part of the haiku response\n",
    "                print(line)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"üèÜ SUCCESS: True distributed MLX inference across 3 Macs!\")\n",
    "        print(f\"‚è±Ô∏è  Total cluster time: {total_time:.1f}s\")\n",
    "        print(\"üéØ Each Mac generated a unique response simultaneously\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Distributed execution failed!\")\n",
    "        print(f\"Return code: {result.returncode}\")\n",
    "        print(f\"Error: {result.stderr[:400]}\")\n",
    "        \n",
    "        if 'permission' in result.stderr.lower() or 'python3.11' in result.stderr:\n",
    "            print(\"\\nüí° Trying alternative approach with conda run...\")\n",
    "            \n",
    "            # Alternative: Use conda run directly\n",
    "            alt_script = f'''#!/bin/bash\n",
    "cd /Users/zz/Documents/GitHub/mlx-dist-setup\n",
    "export MLX_METAL_DEBUG=1\n",
    "~/miniconda3/bin/conda run -n mlx-distributed python real_distributed_inference.py\n",
    "'''\n",
    "            \n",
    "            with open('run_distributed.sh', 'w') as f:\n",
    "                f.write(alt_script)\n",
    "            \n",
    "            # Make executable and deploy\n",
    "            subprocess.run(['chmod', '+x', 'run_distributed.sh'])\n",
    "            \n",
    "            for host in CLUSTER_HOSTS[1:]:\n",
    "                subprocess.run(['scp', 'run_distributed.sh', f'{USER}@{host}:~/run_distributed.sh'])\n",
    "                subprocess.run(['ssh', host, 'chmod +x ~/run_distributed.sh'])\n",
    "            \n",
    "            print(\"üîÑ Deployed alternative launcher - try running cell again\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è  Timeout - the model loading took too long\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Execution error: {e}\")\n",
    "\n",
    "print(f\"\\nüí° Edit the 'custom_prompt' variable above and re-run to try different prompts!\")\n",
    "print(\"üåê This runs TRUE distributed inference across all 3 Macs in your cluster.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-distributed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
