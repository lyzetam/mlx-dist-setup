{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ce8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on 1 processes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dfafd501ca43899e5eb92601cfc325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1 on mbp] I'm a new user on this forum and I'm excited to start my journey here. I've\n"
     ]
    }
   ],
   "source": [
    "# import socket\n",
    "# import mlx.core as mx\n",
    "# from mlx_lm import load, generate\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     world = mx.distributed.init()\n",
    "#     rank = world.rank()\n",
    "#     size = world.size()\n",
    "\n",
    "#     mx.set_default_device(mx.gpu)\n",
    "\n",
    "#     if rank == 0:\n",
    "#         print(f\"Running on {size} processes\")\n",
    "\n",
    "#     model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "#     prompt = f\"Hello from rank {rank}!\"\n",
    "#     result = generate(model, tokenizer, prompt, max_tokens=20)\n",
    "\n",
    "#     print(f\"[{rank}/{size} on {socket.gethostname()}] {result}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0d1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MLX Distributed Inference ===\n",
      "Running on 1 processes\n",
      "Hosts: rank0\n",
      "========================================\n",
      "\n",
      "Loading model on all ranks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d57993f95841e6bf0bc6115f8953ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 0/mbp] Model loaded in 0.69s\n",
      "\n",
      "=== Generating Responses ===\n",
      "\n",
      "[Rank 0/mbp]\n",
      "Prompt: The future of artificial intelligence is\n",
      "Response: not just about creating machines that can think and learn like humans. It's about creating machines that can help us solve some of the world's most pressing problems.\n",
      "\n",
      "From climate change to poverty, from disease to disaster, artificial intelligence has the potential to make\n",
      "Generation time: 0.22s\n",
      "\n",
      "=== Inference Complete ===\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    world = mx.distributed.init()\n",
    "    rank = world.rank()\n",
    "    size = world.size()\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    # Set GPU\n",
    "    mx.set_default_device(mx.gpu)\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"=== MLX Distributed Inference ===\")\n",
    "        print(f\"Running on {size} processes\")\n",
    "        print(f\"Hosts: {', '.join([f'rank{i}' for i in range(size)])}\")\n",
    "        print(\"=\"*40)\n",
    "    \n",
    "    # Each rank loads the model\n",
    "    if rank == 0:\n",
    "        print(\"\\nLoading model on all ranks...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    model, tokenizer = load(\"mlx-community/Llama-3.2-1B-Instruct-4bit\")\n",
    "    load_time = time.time() - start\n",
    "    \n",
    "    print(f\"[Rank {rank}/{hostname}] Model loaded in {load_time:.2f}s\")\n",
    "    \n",
    "    # Synchronize after loading\n",
    "    mx.eval(mx.distributed.all_sum(mx.array([1.0])))\n",
    "    \n",
    "    # Different prompts for each rank\n",
    "    prompts = [\n",
    "        \"The future of artificial intelligence is\",\n",
    "        \"Machine learning helps us to\",\n",
    "        \"The most important technology today is\",\n",
    "        \"Distributed computing enables\",\n",
    "        \"Apple Silicon chips are\",\n",
    "        \"The best programming language is\"\n",
    "    ]\n",
    "    \n",
    "    prompt = prompts[rank % len(prompts)]\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\n=== Generating Responses ===\")\n",
    "    \n",
    "    # Generate response\n",
    "    start = time.time()\n",
    "    result = generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_tokens=50,\n",
    "    )\n",
    "    gen_time = time.time() - start\n",
    "    \n",
    "    # Print results in order\n",
    "    for i in range(size):\n",
    "        if rank == i:\n",
    "            print(f\"\\n[Rank {rank}/{hostname}]\")\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Response: {result}\")\n",
    "            print(f\"Generation time: {gen_time:.2f}s\")\n",
    "        mx.eval(mx.distributed.all_sum(mx.array([1.0]))) \n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"\\n=== Inference Complete ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-distributed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
